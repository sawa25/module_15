{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.23.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install statsmodels==0.11\n",
    "# %pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'resettable_cache'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-25f62a832843>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m from statsmodels.tools.decorators import (cache_readonly,\n\u001b[0m\u001b[0;32m     23\u001b[0m                                           resettable_cache)\n\u001b[0;32m     24\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtsa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtsa_model\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtsbase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'resettable_cache'"
     ]
    }
   ],
   "source": [
    "# Note: The information criteria add 1 to the number of parameters\n",
    "#       whenever the model has an AR or MA term since, in principle,\n",
    "#       the variance could be treated as a free parameter and restricted\n",
    "#       This code does not allow this, but it adds consistency with other\n",
    "#       packages such as gretl and X12-ARIMA\n",
    " \n",
    "from __future__ import absolute_import\n",
    "# from statsmodels.compat.python import string_types, range\n",
    "string_types = str\n",
    "\n",
    "# for 2to3 with extensions\n",
    " \n",
    "from datetime import datetime\n",
    " \n",
    "import numpy as np\n",
    "from scipy import optimize\n",
    "from scipy.stats import t, norm\n",
    "from scipy.signal import lfilter\n",
    "from numpy import dot, log, zeros, pi\n",
    "from numpy.linalg import inv\n",
    " \n",
    "from statsmodels.tools.decorators import (cache_readonly,\n",
    "                                          resettable_cache)\n",
    "import statsmodels.tsa.base.tsa_model as tsbase\n",
    "import statsmodels.base.wrapper as wrap\n",
    "from statsmodels.regression.linear_model import yule_walker, GLS\n",
    "from statsmodels.tsa.tsatools import (lagmat, add_trend,\n",
    "                                      _ar_transparams, _ar_invtransparams,\n",
    "                                      _ma_transparams, _ma_invtransparams,\n",
    "                                      unintegrate, unintegrate_levels)\n",
    "from statsmodels.tsa.vector_ar import util\n",
    "from statsmodels.tsa.ar_model import AR\n",
    "from statsmodels.tsa.arima_process import arma2ma\n",
    "from statsmodels.tools.numdiff import approx_hess_cs, approx_fprime_cs\n",
    "from statsmodels.tsa.base.datetools import _index_date\n",
    "from statsmodels.tsa.kalmanf import KalmanFilter\n",
    " \n",
    "_armax_notes = \"\"\"\n",
    " \n",
    "        Notes\n",
    "        -----\n",
    "        If exogenous variables are given, then the model that is fit is\n",
    " \n",
    "        .. math::\n",
    " \n",
    "           \\\\phi(L)(y_t - X_t\\\\beta) = \\\\theta(L)\\epsilon_t\n",
    " \n",
    "        where :math:`\\\\phi` and :math:`\\\\theta` are polynomials in the lag\n",
    "        operator, :math:`L`. This is the regression model with ARMA errors,\n",
    "        or ARMAX model. This specification is used, whether or not the model\n",
    "        is fit using conditional sum of square or maximum-likelihood, using\n",
    "        the `method` argument in\n",
    "        :meth:`statsmodels.tsa.arima_model.%(Model)s.fit`. Therefore, for\n",
    "        now, `css` and `mle` refer to estimation methods only. This may\n",
    "        change for the case of the `css` model in future versions.\n",
    "\"\"\"\n",
    " \n",
    "_arma_params = \"\"\"\\\n",
    "    endog : array-like\n",
    "        The endogenous variable.\n",
    "    order : iterable\n",
    "        The (p,q) order of the model for the number of AR parameters,\n",
    "        differences, and MA parameters to use.\n",
    "    exog : array-like, optional\n",
    "        An optional arry of exogenous variables. This should *not* include a\n",
    "        constant or trend. You can specify this in the `fit` method.\"\"\"\n",
    " \n",
    "_arma_model = \"Autoregressive Moving Average ARMA(p,q) Model\"\n",
    " \n",
    "_arima_model = \"Autoregressive Integrated Moving Average ARIMA(p,d,q) Model\"\n",
    " \n",
    "_arima_params = \"\"\"\\\n",
    "    endog : array-like\n",
    "        The endogenous variable.\n",
    "    order : iterable\n",
    "        The (p,d,q) order of the model for the number of AR parameters,\n",
    "        differences, and MA parameters to use.\n",
    "    exog : array-like, optional\n",
    "        An optional arry of exogenous variables. This should *not* include a\n",
    "        constant or trend. You can specify this in the `fit` method.\"\"\"\n",
    " \n",
    "_predict_notes = \"\"\"\n",
    "        Notes\n",
    "        -----\n",
    "        Use the results predict method instead.\n",
    "\"\"\"\n",
    " \n",
    "_results_notes = \"\"\"\n",
    "        Notes\n",
    "        -----\n",
    "        It is recommended to use dates with the time-series models, as the\n",
    "        below will probably make clear. However, if ARIMA is used without\n",
    "        dates and/or `start` and `end` are given as indices, then these\n",
    "        indices are in terms of the *original*, undifferenced series. Ie.,\n",
    "        given some undifferenced observations::\n",
    " \n",
    "         1970Q1, 1\n",
    "         1970Q2, 1.5\n",
    "         1970Q3, 1.25\n",
    "         1970Q4, 2.25\n",
    "         1971Q1, 1.2\n",
    "         1971Q2, 4.1\n",
    " \n",
    "        1970Q1 is observation 0 in the original series. However, if we fit an\n",
    "        ARIMA(p,1,q) model then we lose this first observation through\n",
    "        differencing. Therefore, the first observation we can forecast (if\n",
    "        using exact MLE) is index 1. In the differenced series this is index\n",
    "        0, but we refer to it as 1 from the original series.\n",
    "\"\"\"\n",
    " \n",
    "_predict = \"\"\"\n",
    "        %(Model)s model in-sample and out-of-sample prediction\n",
    " \n",
    "        Parameters\n",
    "        ----------\n",
    "        %(params)s\n",
    "        start : int, str, or datetime\n",
    "            Zero-indexed observation number at which to start forecasting, ie.,\n",
    "            the first forecast is start. Can also be a date string to\n",
    "            parse or a datetime type.\n",
    "        end : int, str, or datetime\n",
    "            Zero-indexed observation number at which to end forecasting, ie.,\n",
    "            the first forecast is start. Can also be a date string to\n",
    "            parse or a datetime type. However, if the dates index does not\n",
    "            have a fixed frequency, end must be an integer index if you\n",
    "            want out of sample prediction.\n",
    "        exog : array-like, optional\n",
    "            If the model is an ARMAX and out-of-sample forecasting is\n",
    "            requested, exog must be given. Note that you'll need to pass\n",
    "            `k_ar` additional lags for any exogenous variables. E.g., if you\n",
    "            fit an ARMAX(2, q) model and want to predict 5 steps, you need 7\n",
    "            observations to do this.\n",
    "        dynamic : bool, optional\n",
    "            The `dynamic` keyword affects in-sample prediction. If dynamic\n",
    "            is False, then the in-sample lagged values are used for\n",
    "            prediction. If `dynamic` is True, then in-sample forecasts are\n",
    "            used in place of lagged dependent variables. The first forecasted\n",
    "            value is `start`.\n",
    "        %(extra_params)s\n",
    " \n",
    "        Returns\n",
    "        -------\n",
    "        %(returns)s\n",
    "        %(extra_section)s\n",
    "\"\"\"\n",
    " \n",
    "_predict_returns = \"\"\"predict : array\n",
    "            The predicted values.\n",
    " \n",
    "\"\"\"\n",
    " \n",
    "_arma_predict = _predict % {\"Model\" : \"ARMA\",\n",
    "                            \"params\" : \"\"\"\n",
    "            params : array-like\n",
    "            The fitted parameters of the model.\"\"\",\n",
    "                            \"extra_params\" : \"\",\n",
    "                            \"returns\" : _predict_returns,\n",
    "                            \"extra_section\" : _predict_notes}\n",
    " \n",
    "_arma_results_predict = _predict % {\"Model\" : \"ARMA\", \"params\" : \"\",\n",
    "                                    \"extra_params\" : \"\",\n",
    "                                    \"returns\" : _predict_returns,\n",
    "                                    \"extra_section\" : _results_notes}\n",
    " \n",
    "_arima_predict = _predict % {\"Model\" : \"ARIMA\",\n",
    "                             \"params\" : \"\"\"params : array-like\n",
    "            The fitted parameters of the model.\"\"\",\n",
    "                             \"extra_params\" : \"\"\"typ : str {'linear', 'levels'}\n",
    " \n",
    "            - 'linear' : Linear prediction in terms of the differenced\n",
    "              endogenous variables.\n",
    "            - 'levels' : Predict the levels of the original endogenous\n",
    "              variables.\\n\"\"\", \"returns\" : _predict_returns,\n",
    "                             \"extra_section\" : _predict_notes}\n",
    " \n",
    "_arima_results_predict = _predict % {\"Model\" : \"ARIMA\",\n",
    "                                     \"params\" : \"\",\n",
    "                                     \"extra_params\" :\n",
    "                                     \"\"\"typ : str {'linear', 'levels'}\n",
    " \n",
    "            - 'linear' : Linear prediction in terms of the differenced\n",
    "              endogenous variables.\n",
    "            - 'levels' : Predict the levels of the original endogenous\n",
    "              variables.\\n\"\"\",\n",
    "                                     \"returns\" : _predict_returns,\n",
    "                                     \"extra_section\" : _results_notes}\n",
    " \n",
    "_arima_plot_predict_example = \"\"\"        Examples\n",
    "        --------\n",
    "        >>> import statsmodels.api as sm\n",
    "        >>> import matplotlib.pyplot as plt\n",
    "        >>> import pandas as pd\n",
    "        >>>\n",
    "        >>> dta = sm.datasets.sunspots.load_pandas().data[['SUNACTIVITY']]\n",
    "        >>> dta.index = pd.DatetimeIndex(start='1700', end='2009', freq='A')\n",
    "        >>> res = sm.tsa.ARMA(dta, (3, 0)).fit()\n",
    "        >>> fig, ax = plt.subplots()\n",
    "        >>> ax = dta.ix['1950':].plot(ax=ax)\n",
    "        >>> fig = res.plot_predict('1990', '2012', dynamic=True, ax=ax,\n",
    "        ...                        plot_insample=False)\n",
    "        >>> plt.show()\n",
    " \n",
    "        .. plot:: plots/arma_predict_plot.py\n",
    "\"\"\"\n",
    " \n",
    "_plot_predict = (\"\"\"\n",
    "        Plot forecasts\n",
    "                      \"\"\" + '\\n'.join(_predict.split('\\n')[2:])) % {\n",
    "                      \"params\" : \"\",\n",
    "                          \"extra_params\" : \"\"\"alpha : float, optional\n",
    "            The confidence intervals for the forecasts are (1 - alpha)%\n",
    "        plot_insample : bool, optional\n",
    "            Whether to plot the in-sample series. Default is True.\n",
    "        ax : matplotlib.Axes, optional\n",
    "            Existing axes to plot with.\"\"\",\n",
    "                      \"returns\" : \"\"\"fig : matplotlib.Figure\n",
    "            The plotted Figure instance\"\"\",\n",
    "                      \"extra_section\" : ('\\n' + _arima_plot_predict_example +\n",
    "                                         '\\n' + _results_notes)\n",
    "                      }\n",
    " \n",
    "_arima_plot_predict = (\"\"\"\n",
    "        Plot forecasts\n",
    "                      \"\"\" + '\\n'.join(_predict.split('\\n')[2:])) % {\n",
    "                      \"params\" : \"\",\n",
    "                          \"extra_params\" : \"\"\"alpha : float, optional\n",
    "            The confidence intervals for the forecasts are (1 - alpha)%\n",
    "        plot_insample : bool, optional\n",
    "            Whether to plot the in-sample series. Default is True.\n",
    "        ax : matplotlib.Axes, optional\n",
    "            Existing axes to plot with.\"\"\",\n",
    "                      \"returns\" : \"\"\"fig : matplotlib.Figure\n",
    "            The plotted Figure instance\"\"\",\n",
    "                \"extra_section\" : ('\\n' + _arima_plot_predict_example +\n",
    "                                   '\\n' +\n",
    "                                   '\\n'.join(_results_notes.split('\\n')[:3]) +\n",
    "                              (\"\"\"\n",
    "        This is hard-coded to only allow plotting of the forecasts in levels.\n",
    "\"\"\") +\n",
    "                              '\\n'.join(_results_notes.split('\\n')[3:]))\n",
    "                      }\n",
    " \n",
    " \n",
    "def cumsum_n(x, n):\n",
    "    if n:\n",
    "        n -= 1\n",
    "        x = np.cumsum(x)\n",
    "        return cumsum_n(x, n)\n",
    "    else:\n",
    "        return x\n",
    " \n",
    " \n",
    "def _check_arima_start(start, k_ar, k_diff, method, dynamic):\n",
    "    if start < 0:\n",
    "        raise ValueError(\"The start index %d of the original series \"\n",
    "                         \"has been differenced away\" % start)\n",
    "    elif (dynamic or 'mle' not in method) and start < k_ar:\n",
    "        raise ValueError(\"Start must be >= k_ar for conditional MLE \"\n",
    "                         \"or dynamic forecast. Got %d\" % start)\n",
    " \n",
    " \n",
    "def _get_predict_out_of_sample(endog, p, q, k_trend, k_exog, start, errors,\n",
    "                               trendparam, exparams, arparams, maparams, steps,\n",
    "                               method, exog=None):\n",
    "    \"\"\"\n",
    "    Returns endog, resid, mu of appropriate length for out of sample\n",
    "    prediction.\n",
    "    \"\"\"\n",
    "    if q:\n",
    "        resid = np.zeros(q)\n",
    "        if start and 'mle' in method or (start == p and not start == 0):\n",
    "            resid[:q] = errors[start-q:start]\n",
    "        elif start:\n",
    "            resid[:q] = errors[start-q-p:start-p]\n",
    "        else:\n",
    "            resid[:q] = errors[-q:]\n",
    "    else:\n",
    "        resid = None\n",
    " \n",
    "    y = endog\n",
    "    if k_trend == 1:\n",
    "        # use expectation not constant\n",
    "        if k_exog > 0:\n",
    "            #TODO: technically should only hold for MLE not\n",
    "            # conditional model. See #274.\n",
    "            # ensure 2-d for conformability\n",
    "            if np.ndim(exog) == 1 and k_exog == 1:\n",
    "                # have a 1d series of observations -> 2d\n",
    "                exog = exog[:, None]\n",
    "            elif np.ndim(exog) == 1:\n",
    "                # should have a 1d row of exog -> 2d\n",
    "                if len(exog) != k_exog:\n",
    "                    raise ValueError(\"1d exog given and len(exog) != k_exog\")\n",
    "                exog = exog[None, :]\n",
    "            X = lagmat(np.dot(exog, exparams), p, original='in', trim='both')\n",
    "            mu = trendparam * (1 - arparams.sum())\n",
    "            # arparams were reversed in unpack for ease later\n",
    "            mu = mu + (np.r_[1, -arparams[::-1]] * X).sum(1)[:, None]\n",
    "        else:\n",
    "            mu = trendparam * (1 - arparams.sum())\n",
    "            mu = np.array([mu]*steps)\n",
    "    elif k_exog > 0:\n",
    "        X = np.dot(exog, exparams)\n",
    "        #NOTE: you shouldn't have to give in-sample exog!\n",
    "        X = lagmat(X, p, original='in', trim='both')\n",
    "        mu = (np.r_[1, -arparams[::-1]] * X).sum(1)[:, None]\n",
    "    else:\n",
    "        mu = np.zeros(steps)\n",
    " \n",
    "    endog = np.zeros(p + steps - 1)\n",
    " \n",
    "    if p and start:\n",
    "        endog[:p] = y[start-p:start]\n",
    "    elif p:\n",
    "        endog[:p] = y[-p:]\n",
    " \n",
    "    return endog, resid, mu\n",
    " \n",
    " \n",
    "def _arma_predict_out_of_sample(params, steps, errors, p, q, k_trend, k_exog,\n",
    "                                endog, exog=None, start=0, method='mle'):\n",
    "    (trendparam, exparams,\n",
    "     arparams, maparams) = _unpack_params(params, (p, q), k_trend,\n",
    "                                          k_exog, reverse=True)\n",
    " #   print 'params:',params\n",
    " #   print 'arparams:',arparams,'maparams:',maparams\n",
    "    endog, resid, mu = _get_predict_out_of_sample(endog, p, q, k_trend, k_exog,\n",
    "                                                  start, errors, trendparam,\n",
    "                                                  exparams, arparams,\n",
    "                                                  maparams, steps, method,\n",
    "                                                  exog)\n",
    "#    print 'mu[-1]:',mu[-1], 'mu[0]:',mu[0]\n",
    "    forecast = np.zeros(steps)\n",
    "    if steps == 1:\n",
    "        if q:\n",
    "            return mu[0] + np.dot(arparams, endog[:p]) + np.dot(maparams,\n",
    "                                                                resid[:q]), mu[0]\n",
    "        else:\n",
    "            return mu[0] + np.dot(arparams, endog[:p]), mu[0]\n",
    " \n",
    "    if q:\n",
    "        i = 0  # if q == 1\n",
    "    else:\n",
    "        i = -1\n",
    " \n",
    "    for i in range(min(q, steps - 1)):\n",
    "        fcast = (mu[i] + np.dot(arparams, endog[i:i + p]) +\n",
    "                 np.dot(maparams[:q - i], resid[i:i + q]))\n",
    "        forecast[i] = fcast\n",
    "        endog[i+p] = fcast\n",
    " \n",
    "    for i in range(i + 1, steps - 1):\n",
    "        fcast = mu[i] + np.dot(arparams, endog[i:i+p])\n",
    "        forecast[i] = fcast\n",
    "        endog[i+p] = fcast\n",
    " \n",
    "    #need to do one more without updating endog\n",
    "    forecast[-1] = mu[-1] + np.dot(arparams, endog[steps - 1:])\n",
    "    return forecast, mu[-1] #Modified by me, the former is return forecast\n",
    " \n",
    " \n",
    "def _arma_predict_in_sample(start, end, endog, resid, k_ar, method):\n",
    "    \"\"\"\n",
    "    Pre- and in-sample fitting for ARMA.\n",
    "    \"\"\"\n",
    "    if 'mle' in method:\n",
    "        fittedvalues = endog - resid  # get them all then trim\n",
    "    else:\n",
    "        fittedvalues = endog[k_ar:] - resid\n",
    " \n",
    "    fv_start = start\n",
    "    if 'mle' not in method:\n",
    "        fv_start -= k_ar  # start is in terms of endog index\n",
    "    fv_end = min(len(fittedvalues), end + 1)\n",
    "    return fittedvalues[fv_start:fv_end]\n",
    " \n",
    " \n",
    "def _validate(start, k_ar, k_diff, dates, method):\n",
    "    if isinstance(start, (string_types, datetime)):\n",
    "        start = _index_date(start, dates)\n",
    "        start -= k_diff\n",
    "    if 'mle' not in method and start < k_ar - k_diff:\n",
    "        raise ValueError(\"Start must be >= k_ar for conditional \"\n",
    "                         \"MLE or dynamic forecast. Got %s\" % start)\n",
    " \n",
    "    return start\n",
    " \n",
    " \n",
    "def _unpack_params(params, order, k_trend, k_exog, reverse=False):\n",
    "    p, q = order\n",
    "    k = k_trend + k_exog\n",
    "    maparams = params[k+p:]\n",
    "    arparams = params[k:k+p]\n",
    "    trend = params[:k_trend]\n",
    "    exparams = params[k_trend:k]\n",
    "    if reverse:\n",
    "        return trend, exparams, arparams[::-1], maparams[::-1]\n",
    "    return trend, exparams, arparams, maparams\n",
    " \n",
    " \n",
    "def _unpack_order(order):\n",
    "    k_ar, k_ma, k = order\n",
    "    k_lags = max(k_ar, k_ma+1)\n",
    "    return k_ar, k_ma, order, k_lags\n",
    " \n",
    " \n",
    "def _make_arma_names(data, k_trend, order, exog_names):\n",
    "    k_ar, k_ma = order\n",
    "    exog_names = exog_names or []\n",
    "    ar_lag_names = util.make_lag_names([data.ynames], k_ar, 0)\n",
    "    ar_lag_names = [''.join(('ar.', i)) for i in ar_lag_names]\n",
    "    ma_lag_names = util.make_lag_names([data.ynames], k_ma, 0)\n",
    "    ma_lag_names = [''.join(('ma.', i)) for i in ma_lag_names]\n",
    "    trend_name = util.make_lag_names('', 0, k_trend)\n",
    "    exog_names = trend_name + exog_names + ar_lag_names + ma_lag_names\n",
    "    return exog_names\n",
    " \n",
    " \n",
    "def _make_arma_exog(endog, exog, trend):\n",
    "    k_trend = 1  # overwritten if no constant\n",
    "    if exog is None and trend == 'c':   # constant only\n",
    "        exog = np.ones((len(endog), 1))\n",
    "    elif exog is not None and trend == 'c':  # constant plus exogenous\n",
    "        exog = add_trend(exog, trend='c', prepend=True)\n",
    "    elif exog is not None and trend == 'nc':\n",
    "        # make sure it's not holding constant from last run\n",
    "        if exog.var() == 0:\n",
    "            exog = None\n",
    "        k_trend = 0\n",
    "    if trend == 'nc':\n",
    "        k_trend = 0\n",
    "    return k_trend, exog\n",
    " \n",
    " \n",
    "def _check_estimable(nobs, n_params):\n",
    "    if nobs <= n_params:\n",
    "        raise ValueError(\"Insufficient degrees of freedom to estimate\")\n",
    " \n",
    " \n",
    "class ARMA(tsbase.TimeSeriesModel):\n",
    " \n",
    "    __doc__ = tsbase._tsa_doc % {\"model\" : _arma_model,\n",
    "                                 \"params\" : _arma_params, \"extra_params\" : \"\",\n",
    "                                 \"extra_sections\" : _armax_notes %\n",
    "                                 {\"Model\" : \"ARMA\"}}\n",
    " \n",
    "    def __init__(self, endog, order, exog=None, dates=None, freq=None,\n",
    "                 missing='none'):\n",
    "        super(ARMA, self).__init__(endog, exog, dates, freq, missing=missing)\n",
    "        exog = self.data.exog  # get it after it's gone through processing\n",
    "        _check_estimable(len(self.endog), sum(order))\n",
    "        self.k_ar = k_ar = order[0]\n",
    "        self.k_ma = k_ma = order[1]\n",
    "        self.k_lags = max(k_ar, k_ma+1)\n",
    "        self.constant = 0 #Added by me\n",
    "        if exog is not None:\n",
    "            if exog.ndim == 1:\n",
    "                exog = exog[:, None]\n",
    "            k_exog = exog.shape[1]  # number of exog. variables excl. const\n",
    "        else:\n",
    "            k_exog = 0\n",
    "        self.k_exog = k_exog\n",
    " \n",
    "    def _fit_start_params_hr(self, order):\n",
    "        \"\"\"\n",
    "        Get starting parameters for fit.\n",
    " \n",
    "        Parameters\n",
    "        ----------\n",
    "        order : iterable\n",
    "            (p,q,k) - AR lags, MA lags, and number of exogenous variables\n",
    "            including the constant.\n",
    " \n",
    "        Returns\n",
    "        -------\n",
    "        start_params : array\n",
    "            A first guess at the starting parameters.\n",
    " \n",
    "        Notes\n",
    "        -----\n",
    "        If necessary, fits an AR process with the laglength selected according\n",
    "        to best BIC.  Obtain the residuals.  Then fit an ARMA(p,q) model via\n",
    "        OLS using these residuals for a first approximation.  Uses a separate\n",
    "        OLS regression to find the coefficients of exogenous variables.\n",
    " \n",
    "        References\n",
    "        ----------\n",
    "        Hannan, E.J. and Rissanen, J.  1982.  \"Recursive estimation of mixed\n",
    "            autoregressive-moving average order.\"  `Biometrika`.  69.1.\n",
    "        \"\"\"\n",
    "        p, q, k = order\n",
    "        start_params = zeros((p+q+k))\n",
    "        endog = self.endog.copy()  # copy because overwritten\n",
    "        exog = self.exog\n",
    "        if k != 0:\n",
    "            ols_params = GLS(endog, exog).fit().params\n",
    "            start_params[:k] = ols_params\n",
    "            endog -= np.dot(exog, ols_params).squeeze()\n",
    "        if q != 0:\n",
    "            if p != 0:\n",
    "                # make sure we don't run into small data problems in AR fit\n",
    "                nobs = len(endog)\n",
    "                maxlag = int(round(12*(nobs/100.)**(1/4.)))\n",
    "                if maxlag >= nobs:\n",
    "                    maxlag = nobs - 1\n",
    "                armod = AR(endog).fit(ic='bic', trend='nc', maxlag=maxlag)\n",
    "                arcoefs_tmp = armod.params\n",
    "                p_tmp = armod.k_ar\n",
    "                # it's possible in small samples that optimal lag-order\n",
    "                # doesn't leave enough obs. No consistent way to fix.\n",
    "                if p_tmp + q >= len(endog):\n",
    "                    raise ValueError(\"Proper starting parameters cannot\"\n",
    "                                     \" be found for this order with this \"\n",
    "                                     \"number of observations. Use the \"\n",
    "                                     \"start_params argument.\")\n",
    "                resid = endog[p_tmp:] - np.dot(lagmat(endog, p_tmp,\n",
    "                                                      trim='both'),\n",
    "                                               arcoefs_tmp)\n",
    "                if p < p_tmp + q:\n",
    "                    endog_start = p_tmp + q - p\n",
    "                    resid_start = 0\n",
    "                else:\n",
    "                    endog_start = 0\n",
    "                    resid_start = p - p_tmp - q\n",
    "                lag_endog = lagmat(endog, p, 'both')[endog_start:]\n",
    "                lag_resid = lagmat(resid, q, 'both')[resid_start:]\n",
    "                # stack ar lags and resids\n",
    "                X = np.column_stack((lag_endog, lag_resid))\n",
    "                coefs = GLS(endog[max(p_tmp + q, p):], X).fit().params\n",
    "                start_params[k:k+p+q] = coefs\n",
    "            else:\n",
    "                start_params[k+p:k+p+q] = yule_walker(endog, order=q)[0]\n",
    "        if q == 0 and p != 0:\n",
    "            arcoefs = yule_walker(endog, order=p)[0]\n",
    "            start_params[k:k+p] = arcoefs\n",
    " \n",
    "        # check AR coefficients\n",
    "        if p and not np.all(np.abs(np.roots(np.r_[1, -start_params[k:k + p]]\n",
    "                                            )) < 1):\n",
    "            raise ValueError(\"The computed initial AR coefficients are not \"\n",
    "                             \"stationary\\nYou should induce stationarity, \"\n",
    "                             \"choose a different model order, or you can\\n\"\n",
    "                             \"pass your own start_params.\")\n",
    "        # check MA coefficients\n",
    "        elif q and not np.all(np.abs(np.roots(np.r_[1, start_params[k + p:]]\n",
    "                                              )) < 1):\n",
    "            return np.zeros(len(start_params))   #modified by me\n",
    "            raise ValueError(\"The computed initial MA coefficients are not \"\n",
    "                             \"invertible\\nYou should induce invertibility, \"\n",
    "                             \"choose a different model order, or you can\\n\"\n",
    "                             \"pass your own start_params.\")\n",
    " \n",
    "        # check MA coefficients\n",
    "        # print start_params\n",
    "        return start_params\n",
    " \n",
    "    def _fit_start_params(self, order, method):\n",
    "        if method != 'css-mle':  # use Hannan-Rissanen to get start params\n",
    "            start_params = self._fit_start_params_hr(order)\n",
    "        else:  # use CSS to get start params\n",
    "            func = lambda params: -self.loglike_css(params)\n",
    "            #start_params = [.1]*(k_ar+k_ma+k_exog) # different one for k?\n",
    "            start_params = self._fit_start_params_hr(order)\n",
    "            if self.transparams:\n",
    "                start_params = self._invtransparams(start_params)\n",
    "            bounds = [(None,)*2]*sum(order)\n",
    "            mlefit = optimize.fmin_l_bfgs_b(func, start_params,\n",
    "                                            approx_grad=True, m=12,\n",
    "                                            pgtol=1e-7, factr=1e3,\n",
    "                                            bounds=bounds, iprint=-1)\n",
    "            start_params = self._transparams(mlefit[0])\n",
    "        return start_params\n",
    " \n",
    "    def score(self, params):\n",
    "        \"\"\"\n",
    "        Compute the score function at params.\n",
    " \n",
    "        Notes\n",
    "        -----\n",
    "        This is a numerical approximation.\n",
    "        \"\"\"\n",
    "        return approx_fprime_cs(params, self.loglike, args=(False,))\n",
    " \n",
    "    def hessian(self, params):\n",
    "        \"\"\"\n",
    "        Compute the Hessian at params,\n",
    " \n",
    "        Notes\n",
    "        -----\n",
    "        This is a numerical approximation.\n",
    "        \"\"\"\n",
    "        return approx_hess_cs(params, self.loglike, args=(False,))\n",
    " \n",
    "    def _transparams(self, params):\n",
    "        \"\"\"\n",
    "        Transforms params to induce stationarity/invertability.\n",
    " \n",
    "        Reference\n",
    "        ---------\n",
    "        Jones(1980)\n",
    "        \"\"\"\n",
    "        k_ar, k_ma = self.k_ar, self.k_ma\n",
    "        k = self.k_exog + self.k_trend\n",
    "        newparams = np.zeros_like(params)\n",
    " \n",
    "        # just copy exogenous parameters\n",
    "        if k != 0:\n",
    "            newparams[:k] = params[:k]\n",
    " \n",
    "        # AR Coeffs\n",
    "        if k_ar != 0:\n",
    "            newparams[k:k+k_ar] = _ar_transparams(params[k:k+k_ar].copy())\n",
    " \n",
    "        # MA Coeffs\n",
    "        if k_ma != 0:\n",
    "            newparams[k+k_ar:] = _ma_transparams(params[k+k_ar:].copy())\n",
    "        return newparams\n",
    " \n",
    "    def _invtransparams(self, start_params):\n",
    "        \"\"\"\n",
    "        Inverse of the Jones reparameterization\n",
    "        \"\"\"\n",
    "        k_ar, k_ma = self.k_ar, self.k_ma\n",
    "        k = self.k_exog + self.k_trend\n",
    "        newparams = start_params.copy()\n",
    "        arcoefs = newparams[k:k+k_ar]\n",
    "        macoefs = newparams[k+k_ar:]\n",
    "        # AR coeffs\n",
    "        if k_ar != 0:\n",
    "            newparams[k:k+k_ar] = _ar_invtransparams(arcoefs)\n",
    " \n",
    "        # MA coeffs\n",
    "        if k_ma != 0:\n",
    "            newparams[k+k_ar:k+k_ar+k_ma] = _ma_invtransparams(macoefs)\n",
    "        return newparams\n",
    " \n",
    "    def _get_predict_start(self, start, dynamic):\n",
    "        # do some defaults\n",
    "        method = getattr(self, 'method', 'mle')\n",
    "        k_ar = getattr(self, 'k_ar', 0)\n",
    "        k_diff = getattr(self, 'k_diff', 0)\n",
    "        if start is None:\n",
    "            if 'mle' in method and not dynamic:\n",
    "                start = 0\n",
    "            else:\n",
    "                start = k_ar\n",
    "            self._set_predict_start_date(start)  # else it's done in super\n",
    "        elif isinstance(start, int):\n",
    "            start = super(ARMA, self)._get_predict_start(start)\n",
    "        else:  # should be on a date\n",
    "            #elif 'mle' not in method or dynamic: # should be on a date\n",
    "            start = _validate(start, k_ar, k_diff, self.data.dates,\n",
    "                              method)\n",
    "            start = super(ARMA, self)._get_predict_start(start)\n",
    "        _check_arima_start(start, k_ar, k_diff, method, dynamic)\n",
    "        return start\n",
    " \n",
    "    def _get_predict_end(self, end, dynamic=False):\n",
    "        # pass through so predict works for ARIMA and ARMA\n",
    "        return super(ARMA, self)._get_predict_end(end)\n",
    " \n",
    "    def geterrors(self, params):\n",
    "        \"\"\"\n",
    "        Get the errors of the ARMA process.\n",
    " \n",
    "        Parameters\n",
    "        ----------\n",
    "        params : array-like\n",
    "            The fitted ARMA parameters\n",
    "        order : array-like\n",
    "            3 item iterable, with the number of AR, MA, and exogenous\n",
    "            parameters, including the trend\n",
    "        \"\"\"\n",
    " \n",
    "        #start = self._get_predict_start(start) # will be an index of a date\n",
    "        #end, out_of_sample = self._get_predict_end(end)\n",
    "        params = np.asarray(params)\n",
    "        k_ar, k_ma = self.k_ar, self.k_ma\n",
    "        k = self.k_exog + self.k_trend\n",
    " \n",
    "        method = getattr(self, 'method', 'mle')\n",
    "        if 'mle' in method:  # use KalmanFilter to get errors\n",
    "            (y, k, nobs, k_ar, k_ma, k_lags, newparams, Z_mat, m, R_mat,\n",
    "             T_mat, paramsdtype) = KalmanFilter._init_kalman_state(params,\n",
    "                                                                   self)\n",
    " \n",
    "            errors = KalmanFilter.geterrors(y, k, k_ar, k_ma, k_lags, nobs,\n",
    "                                            Z_mat, m, R_mat, T_mat,\n",
    "                                            paramsdtype)\n",
    "            if isinstance(errors, tuple):\n",
    "                errors = errors[0]  # non-cython version returns a tuple\n",
    "        else:  # use scipy.signal.lfilter\n",
    "            y = self.endog.copy()\n",
    "            k = self.k_exog + self.k_trend\n",
    "            if k > 0:\n",
    "                y -= dot(self.exog, params[:k])\n",
    " \n",
    "            k_ar = self.k_ar\n",
    "            k_ma = self.k_ma\n",
    " \n",
    "            (trendparams, exparams,\n",
    "             arparams, maparams) = _unpack_params(params, (k_ar, k_ma),\n",
    "                                                  self.k_trend, self.k_exog,\n",
    "                                                  reverse=False)\n",
    "            b, a = np.r_[1, -arparams], np.r_[1, maparams]\n",
    "            zi = zeros((max(k_ar, k_ma)))\n",
    "            for i in range(k_ar):\n",
    "                zi[i] = sum(-b[:i+1][::-1]*y[:i+1])\n",
    "            e = lfilter(b, a, y, zi=zi)\n",
    "            errors = e[0][k_ar:]\n",
    "        return errors.squeeze()\n",
    " \n",
    "    def predict(self, params, start=None, end=None, exog=None, dynamic=False):\n",
    "        method = getattr(self, 'method', 'mle')  # don't assume fit\n",
    "        #params = np.asarray(params)\n",
    " \n",
    "        # will return an index of a date\n",
    "        start = self._get_predict_start(start, dynamic)\n",
    "        end, out_of_sample = self._get_predict_end(end, dynamic)\n",
    "        if out_of_sample and (exog is None and self.k_exog > 0):\n",
    "            raise ValueError(\"You must provide exog for ARMAX\")\n",
    " \n",
    "        endog = self.endog\n",
    "        resid = self.geterrors(params)\n",
    "        k_ar = self.k_ar\n",
    " \n",
    "        if out_of_sample != 0 and self.k_exog > 0:\n",
    "            if self.k_exog == 1 and exog.ndim == 1:\n",
    "                exog = exog[:, None]\n",
    "                # we need the last k_ar exog for the lag-polynomial\n",
    "            if self.k_exog > 0 and k_ar > 0:\n",
    "                # need the last k_ar exog for the lag-polynomial\n",
    "                exog = np.vstack((self.exog[-k_ar:, self.k_trend:], exog))\n",
    " \n",
    "        if dynamic:\n",
    "            #TODO: now that predict does dynamic in-sample it should\n",
    "            # also return error estimates and confidence intervals\n",
    "            # but how? len(endog) is not tot_obs\n",
    "            out_of_sample += end - start + 1\n",
    "            pr, ct = _arma_predict_out_of_sample(params, out_of_sample, resid,\n",
    "                                               k_ar, self.k_ma, self.k_trend,\n",
    "                                               self.k_exog, endog, exog,\n",
    "                                               start, method)\n",
    "            self.constant = ct\n",
    "            return pr\n",
    " \n",
    "        predictedvalues = _arma_predict_in_sample(start, end, endog, resid,\n",
    "                                                  k_ar, method)\n",
    "        if out_of_sample:\n",
    "            forecastvalues, ct = _arma_predict_out_of_sample(params, out_of_sample,\n",
    "                                                         resid, k_ar,\n",
    "                                                         self.k_ma,\n",
    "                                                         self.k_trend,\n",
    "                                                         self.k_exog, endog,\n",
    "                                                         exog, method=method)\n",
    "            self.constant = ct\n",
    "            predictedvalues = np.r_[predictedvalues, forecastvalues]\n",
    "        return predictedvalues\n",
    "    predict.__doc__ = _arma_predict\n",
    " \n",
    "    def loglike(self, params, set_sigma2=True):\n",
    "        \"\"\"\n",
    "        Compute the log-likelihood for ARMA(p,q) model\n",
    " \n",
    "        Notes\n",
    "        -----\n",
    "        Likelihood used depends on the method set in fit\n",
    "        \"\"\"\n",
    "        method = self.method\n",
    "        if method in ['mle', 'css-mle']:\n",
    "            return self.loglike_kalman(params, set_sigma2)\n",
    "        elif method == 'css':\n",
    "            return self.loglike_css(params, set_sigma2)\n",
    "        else:\n",
    "            raise ValueError(\"Method %s not understood\" % method)\n",
    " \n",
    "    def loglike_kalman(self, params, set_sigma2=True):\n",
    "        \"\"\"\n",
    "        Compute exact loglikelihood for ARMA(p,q) model by the Kalman Filter.\n",
    "        \"\"\"\n",
    "        return KalmanFilter.loglike(params, self, set_sigma2)\n",
    " \n",
    "    def loglike_css(self, params, set_sigma2=True):\n",
    "        \"\"\"\n",
    "        Conditional Sum of Squares likelihood function.\n",
    "        \"\"\"\n",
    "        k_ar = self.k_ar\n",
    "        k_ma = self.k_ma\n",
    "        k = self.k_exog + self.k_trend\n",
    "        y = self.endog.copy().astype(params.dtype)\n",
    "        nobs = self.nobs\n",
    "        # how to handle if empty?\n",
    "        if self.transparams:\n",
    "            newparams = self._transparams(params)\n",
    "        else:\n",
    "            newparams = params\n",
    "        if k > 0:\n",
    "            y -= dot(self.exog, newparams[:k])\n",
    "        # the order of p determines how many zeros errors to set for lfilter\n",
    "        b, a = np.r_[1, -newparams[k:k + k_ar]], np.r_[1, newparams[k + k_ar:]]\n",
    "        zi = np.zeros((max(k_ar, k_ma)), dtype=params.dtype)\n",
    "        for i in range(k_ar):\n",
    "            zi[i] = sum(-b[:i + 1][::-1] * y[:i + 1])\n",
    "        errors = lfilter(b, a, y, zi=zi)[0][k_ar:]\n",
    " \n",
    "        ssr = np.dot(errors, errors)\n",
    "        sigma2 = ssr/nobs\n",
    "        if set_sigma2:\n",
    "            self.sigma2 = sigma2\n",
    "        llf = -nobs/2.*(log(2*pi) + log(sigma2)) - ssr/(2*sigma2)\n",
    "        return llf\n",
    " \n",
    "    def fit(self, start_params=None, trend='c', method=\"css-mle\",\n",
    "            transparams=True, solver='lbfgs', maxiter=50, full_output=1,\n",
    "            disp=5, callback=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Fits ARMA(p,q) model using exact maximum likelihood via Kalman filter.\n",
    " \n",
    "        Parameters\n",
    "        ----------\n",
    "        start_params : array-like, optional\n",
    "            Starting parameters for ARMA(p,q). If None, the default is given\n",
    "            by ARMA._fit_start_params.  See there for more information.\n",
    "        transparams : bool, optional\n",
    "            Whehter or not to transform the parameters to ensure stationarity.\n",
    "            Uses the transformation suggested in Jones (1980).  If False,\n",
    "            no checking for stationarity or invertibility is done.\n",
    "        method : str {'css-mle','mle','css'}\n",
    "            This is the loglikelihood to maximize.  If \"css-mle\", the\n",
    "            conditional sum of squares likelihood is maximized and its values\n",
    "            are used as starting values for the computation of the exact\n",
    "            likelihood via the Kalman filter.  If \"mle\", the exact likelihood\n",
    "            is maximized via the Kalman Filter.  If \"css\" the conditional sum\n",
    "            of squares likelihood is maximized.  All three methods use\n",
    "            `start_params` as starting parameters.  See above for more\n",
    "            information.\n",
    "        trend : str {'c','nc'}\n",
    "            Whether to include a constant or not.  'c' includes constant,\n",
    "            'nc' no constant.\n",
    "        solver : str or None, optional\n",
    "            Solver to be used.  The default is 'lbfgs' (limited memory\n",
    "            Broyden-Fletcher-Goldfarb-Shanno).  Other choices are 'bfgs',\n",
    "            'newton' (Newton-Raphson), 'nm' (Nelder-Mead), 'cg' -\n",
    "            (conjugate gradient), 'ncg' (non-conjugate gradient), and\n",
    "            'powell'. By default, the limited memory BFGS uses m=12 to\n",
    "            approximate the Hessian, projected gradient tolerance of 1e-8 and\n",
    "            factr = 1e2. You can change these by using kwargs.\n",
    "        maxiter : int, optional\n",
    "            The maximum number of function evaluations. Default is 50.\n",
    "        tol : float\n",
    "            The convergence tolerance.  Default is 1e-08.\n",
    "        full_output : bool, optional\n",
    "            If True, all output from solver will be available in\n",
    "            the Results object's mle_retvals attribute.  Output is dependent\n",
    "            on the solver.  See Notes for more information.\n",
    "        disp : bool, optional\n",
    "            If True, convergence information is printed.  For the default\n",
    "            l_bfgs_b solver, disp controls the frequency of the output during\n",
    "            the iterations. disp < 0 means no output in this case.\n",
    "        callback : function, optional\n",
    "            Called after each iteration as callback(xk) where xk is the current\n",
    "            parameter vector.\n",
    "        kwargs\n",
    "            See Notes for keyword arguments that can be passed to fit.\n",
    " \n",
    "        Returns\n",
    "        -------\n",
    "        statsmodels.tsa.arima_model.ARMAResults class\n",
    " \n",
    "        See also\n",
    "        --------\n",
    "        statsmodels.base.model.LikelihoodModel.fit : for more information\n",
    "            on using the solvers.\n",
    "        ARMAResults : results class returned by fit\n",
    " \n",
    "        Notes\n",
    "        ------\n",
    "        If fit by 'mle', it is assumed for the Kalman Filter that the initial\n",
    "        unkown state is zero, and that the inital variance is\n",
    "        P = dot(inv(identity(m**2)-kron(T,T)),dot(R,R.T).ravel('F')).reshape(r,\n",
    "        r, order = 'F')\n",
    " \n",
    "        \"\"\"\n",
    "        k_ar = self.k_ar\n",
    "        k_ma = self.k_ma\n",
    " \n",
    "        # enforce invertibility\n",
    "        self.transparams = transparams\n",
    " \n",
    "        endog, exog = self.endog, self.exog\n",
    "        k_exog = self.k_exog\n",
    "        self.nobs = len(endog)  # this is overwritten if method is 'css'\n",
    " \n",
    "        # (re)set trend and handle exogenous variables\n",
    "        # always pass original exog\n",
    "        k_trend, exog = _make_arma_exog(endog, self.exog, trend)\n",
    " \n",
    "        # Check has something to estimate\n",
    "        if k_ar == 0 and k_ma == 0 and k_trend == 0 and k_exog == 0:\n",
    "            raise ValueError(\"Estimation requires the inclusion of least one \"\n",
    "                         \"AR term, MA term, a constant or an exogenous \"\n",
    "                         \"variable.\")\n",
    " \n",
    "        # check again now that we know the trend\n",
    "        _check_estimable(len(endog), k_ar + k_ma + k_exog + k_trend)\n",
    " \n",
    "        self.k_trend = k_trend\n",
    "        self.exog = exog    # overwrites original exog from __init__\n",
    " \n",
    "        # (re)set names for this model\n",
    "        self.exog_names = _make_arma_names(self.data, k_trend, (k_ar, k_ma),\n",
    "                                           self.exog_names)\n",
    "        k = k_trend + k_exog\n",
    " \n",
    "        # choose objective function\n",
    "        if k_ma == 0 and k_ar == 0:\n",
    "            method = \"css\"  # Always CSS when no AR or MA terms\n",
    " \n",
    "        self.method = method = method.lower()\n",
    " \n",
    "        # adjust nobs for css\n",
    "        if method == 'css':\n",
    "            self.nobs = len(self.endog) - k_ar\n",
    " \n",
    "        if start_params is not None:\n",
    "            start_params = np.asarray(start_params)\n",
    " \n",
    "        else:  # estimate starting parameters\n",
    "            start_params = self._fit_start_params((k_ar, k_ma, k), method)\n",
    " \n",
    "        if transparams:  # transform initial parameters to ensure invertibility\n",
    "            start_params = self._invtransparams(start_params)\n",
    " \n",
    "        if solver == 'lbfgs':\n",
    "            kwargs.setdefault('pgtol', 1e-8)\n",
    "            kwargs.setdefault('factr', 1e2)\n",
    "            kwargs.setdefault('m', 12)\n",
    "            kwargs.setdefault('approx_grad', True)\n",
    "        mlefit = super(ARMA, self).fit(start_params, method=solver,\n",
    "                                       maxiter=maxiter,\n",
    "                                       full_output=full_output, disp=disp,\n",
    "                                       callback=callback, **kwargs)\n",
    "        params = mlefit.params\n",
    " \n",
    "        if transparams:  # transform parameters back\n",
    "            params = self._transparams(params)\n",
    " \n",
    "        self.transparams = False  # so methods don't expect transf.\n",
    " \n",
    "        normalized_cov_params = None  # TODO: fix this\n",
    "        armafit = ARMAResults(self, params, normalized_cov_params)\n",
    "        armafit.mle_retvals = mlefit.mle_retvals\n",
    "        armafit.mle_settings = mlefit.mle_settings\n",
    "        armafit.mlefit = mlefit\n",
    "        return ARMAResultsWrapper(armafit)\n",
    " \n",
    " \n",
    "#NOTE: the length of endog changes when we give a difference to fit\n",
    "#so model methods are not the same on unfit models as fit ones\n",
    "#starting to think that order of model should be put in instantiation...\n",
    "class ARIMA(ARMA):\n",
    " \n",
    "    __doc__ = tsbase._tsa_doc % {\"model\" : _arima_model,\n",
    "                                 \"params\" : _arima_params, \"extra_params\" : \"\",\n",
    "                                 \"extra_sections\" : _armax_notes %\n",
    "                                 {\"Model\" : \"ARIMA\"}}\n",
    " \n",
    "    def __new__(cls, endog, order, exog=None, dates=None, freq=None,\n",
    "                missing='none'):\n",
    "        p, d, q = order\n",
    "        if d == 0:  # then we just use an ARMA model\n",
    "            return ARMA(endog, (p, q), exog, dates, freq, missing)\n",
    "        else:\n",
    "            mod = super(ARIMA, cls).__new__(cls)\n",
    "            mod.__init__(endog, order, exog, dates, freq, missing)\n",
    "            return mod\n",
    " \n",
    "    def __init__(self, endog, order, exog=None, dates=None, freq=None,\n",
    "                 missing='none'):\n",
    "        p, d, q = order\n",
    "        if d > 2:\n",
    "            #NOTE: to make more general, need to address the d == 2 stuff\n",
    "            # in the predict method\n",
    "            raise ValueError(\"d > 2 is not supported\")\n",
    "        super(ARIMA, self).__init__(endog, (p, q), exog, dates, freq, missing)\n",
    "        self.k_diff = d\n",
    "        self._first_unintegrate = unintegrate_levels(self.endog[:d], d)\n",
    "        self.endog = np.diff(self.endog, n=d)\n",
    "        #NOTE: will check in ARMA but check again since differenced now\n",
    "        _check_estimable(len(self.endog), p+q)\n",
    "        if exog is not None:\n",
    "            self.exog = self.exog[d:]\n",
    "        if d == 1:\n",
    "            self.data.ynames = 'D.' + self.endog_names\n",
    "        else:\n",
    "            self.data.ynames = 'D{0:d}.'.format(d) + self.endog_names\n",
    "        # what about exog, should we difference it automatically before\n",
    "        # super call?\n",
    " \n",
    "    def _get_predict_start(self, start, dynamic):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        #TODO: remove all these getattr and move order specification to\n",
    "        # class constructor\n",
    "        k_diff = getattr(self, 'k_diff', 0)\n",
    "        method = getattr(self, 'method', 'mle')\n",
    "        k_ar = getattr(self, 'k_ar', 0)\n",
    "        if start is None:\n",
    "            if 'mle' in method and not dynamic:\n",
    "                start = 0\n",
    "            else:\n",
    "                start = k_ar\n",
    "        elif isinstance(start, int):\n",
    "                start -= k_diff\n",
    "                try:  # catch when given an integer outside of dates index\n",
    "                    start = super(ARIMA, self)._get_predict_start(start,\n",
    "                                                                  dynamic)\n",
    "                except IndexError:\n",
    "                    raise ValueError(\"start must be in series. \"\n",
    "                                     \"got %d\" % (start + k_diff))\n",
    "        else:  # received a date\n",
    "            start = _validate(start, k_ar, k_diff, self.data.dates,\n",
    "                              method)\n",
    "            start = super(ARIMA, self)._get_predict_start(start, dynamic)\n",
    "        # reset date for k_diff adjustment\n",
    "        self._set_predict_start_date(start + k_diff)\n",
    "        return start\n",
    " \n",
    "    def _get_predict_end(self, end, dynamic=False):\n",
    "        \"\"\"\n",
    "        Returns last index to be forecast of the differenced array.\n",
    "        Handling of inclusiveness should be done in the predict function.\n",
    "        \"\"\"\n",
    "        end, out_of_sample = super(ARIMA, self)._get_predict_end(end, dynamic)\n",
    "        if 'mle' not in self.method and not dynamic:\n",
    "            end -= self.k_ar\n",
    " \n",
    "        return end - self.k_diff, out_of_sample\n",
    " \n",
    "    def fit(self, start_params=None, trend='c', method=\"css-mle\",\n",
    "            transparams=True, solver='lbfgs', maxiter=50, full_output=1,\n",
    "            disp=5, callback=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Fits ARIMA(p,d,q) model by exact maximum likelihood via Kalman filter.\n",
    " \n",
    "        Parameters\n",
    "        ----------\n",
    "        start_params : array-like, optional\n",
    "            Starting parameters for ARMA(p,q).  If None, the default is given\n",
    "            by ARMA._fit_start_params.  See there for more information.\n",
    "        transparams : bool, optional\n",
    "            Whehter or not to transform the parameters to ensure stationarity.\n",
    "            Uses the transformation suggested in Jones (1980).  If False,\n",
    "            no checking for stationarity or invertibility is done.\n",
    "        method : str {'css-mle','mle','css'}\n",
    "            This is the loglikelihood to maximize.  If \"css-mle\", the\n",
    "            conditional sum of squares likelihood is maximized and its values\n",
    "            are used as starting values for the computation of the exact\n",
    "            likelihood via the Kalman filter.  If \"mle\", the exact likelihood\n",
    "            is maximized via the Kalman Filter.  If \"css\" the conditional sum\n",
    "            of squares likelihood is maximized.  All three methods use\n",
    "            `start_params` as starting parameters.  See above for more\n",
    "            information.\n",
    "        trend : str {'c','nc'}\n",
    "            Whether to include a constant or not.  'c' includes constant,\n",
    "            'nc' no constant.\n",
    "        solver : str or None, optional\n",
    "            Solver to be used.  The default is 'lbfgs' (limited memory\n",
    "            Broyden-Fletcher-Goldfarb-Shanno).  Other choices are 'bfgs',\n",
    "            'newton' (Newton-Raphson), 'nm' (Nelder-Mead), 'cg' -\n",
    "            (conjugate gradient), 'ncg' (non-conjugate gradient), and\n",
    "            'powell'. By default, the limited memory BFGS uses m=12 to\n",
    "            approximate the Hessian, projected gradient tolerance of 1e-8 and\n",
    "            factr = 1e2. You can change these by using kwargs.\n",
    "        maxiter : int, optional\n",
    "            The maximum number of function evaluations. Default is 50.\n",
    "        tol : float\n",
    "            The convergence tolerance.  Default is 1e-08.\n",
    "        full_output : bool, optional\n",
    "            If True, all output from solver will be available in\n",
    "            the Results object's mle_retvals attribute.  Output is dependent\n",
    "            on the solver.  See Notes for more information.\n",
    "        disp : bool, optional\n",
    "            If True, convergence information is printed.  For the default\n",
    "            l_bfgs_b solver, disp controls the frequency of the output during\n",
    "            the iterations. disp < 0 means no output in this case.\n",
    "        callback : function, optional\n",
    "            Called after each iteration as callback(xk) where xk is the current\n",
    "            parameter vector.\n",
    "        kwargs\n",
    "            See Notes for keyword arguments that can be passed to fit.\n",
    " \n",
    "        Returns\n",
    "        -------\n",
    "        `statsmodels.tsa.arima.ARIMAResults` class\n",
    " \n",
    "        See also\n",
    "        --------\n",
    "        statsmodels.base.model.LikelihoodModel.fit : for more information\n",
    "            on using the solvers.\n",
    "        ARIMAResults : results class returned by fit\n",
    " \n",
    "        Notes\n",
    "        ------\n",
    "        If fit by 'mle', it is assumed for the Kalman Filter that the initial\n",
    "        unkown state is zero, and that the inital variance is\n",
    "        P = dot(inv(identity(m**2)-kron(T,T)),dot(R,R.T).ravel('F')).reshape(r,\n",
    "        r, order = 'F')\n",
    " \n",
    "        \"\"\"\n",
    "        arima_fit = super(ARIMA, self).fit(start_params, trend,\n",
    "                                           method, transparams, solver,\n",
    "                                           maxiter, full_output, disp,\n",
    "                                           callback, **kwargs)\n",
    "        normalized_cov_params = None  # TODO: fix this?\n",
    "        arima_fit = ARIMAResults(self, arima_fit._results.params,\n",
    "                                 normalized_cov_params)\n",
    "        arima_fit.k_diff = self.k_diff\n",
    "        return ARIMAResultsWrapper(arima_fit)\n",
    " \n",
    "    def predict(self, params, start=None, end=None, exog=None, typ='linear',\n",
    "                dynamic=False):\n",
    "        # go ahead and convert to an index for easier checking\n",
    "        if isinstance(start, (string_types, datetime)):\n",
    "            start = _index_date(start, self.data.dates)\n",
    "        if typ == 'linear':\n",
    "            if not dynamic or (start != self.k_ar + self.k_diff and\n",
    "                               start is not None):\n",
    "                return super(ARIMA, self).predict(params, start, end, exog,\n",
    "                                                  dynamic)\n",
    "            else:\n",
    "                # need to assume pre-sample residuals are zero\n",
    "                # do this by a hack\n",
    "                q = self.k_ma\n",
    "                self.k_ma = 0\n",
    "                predictedvalues = super(ARIMA, self).predict(params, start,\n",
    "                                                             end, exog,\n",
    "                                                             dynamic)\n",
    "                self.k_ma = q\n",
    "                return predictedvalues\n",
    "        elif typ == 'levels':\n",
    "            endog = self.data.endog\n",
    "            if not dynamic:\n",
    "                predict = super(ARIMA, self).predict(params, start, end,\n",
    "                                                     dynamic)\n",
    " \n",
    "                start = self._get_predict_start(start, dynamic)\n",
    "                end, out_of_sample = self._get_predict_end(end)\n",
    "                d = self.k_diff\n",
    "                if 'mle' in self.method:\n",
    "                    start += d - 1  # for case where d == 2\n",
    "                    end += d - 1\n",
    "                    # add each predicted diff to lagged endog\n",
    "                    if out_of_sample:\n",
    "                        fv = predict[:-out_of_sample] + endog[start:end+1]\n",
    "                        if d == 2:  #TODO: make a general solution to this\n",
    "                            fv += np.diff(endog[start - 1:end + 1])\n",
    "                        levels = unintegrate_levels(endog[-d:], d)\n",
    "                        fv = np.r_[fv,\n",
    "                                   unintegrate(predict[-out_of_sample:],\n",
    "                                               levels)[d:]]\n",
    "                    else:\n",
    "                        fv = predict + endog[start:end + 1]\n",
    "                        if d == 2:\n",
    "                            fv += np.diff(endog[start - 1:end + 1])\n",
    "                else:\n",
    "                    k_ar = self.k_ar\n",
    "                    if out_of_sample:\n",
    "                        fv = (predict[:-out_of_sample] +\n",
    "                              endog[max(start, self.k_ar-1):end+k_ar+1])\n",
    "                        if d == 2:\n",
    "                            fv += np.diff(endog[start - 1:end + 1])\n",
    "                        levels = unintegrate_levels(endog[-d:], d)\n",
    "                        fv = np.r_[fv,\n",
    "                                   unintegrate(predict[-out_of_sample:],\n",
    "                                               levels)[d:]]\n",
    "                    else:\n",
    "                        fv = predict + endog[max(start, k_ar):end+k_ar+1]\n",
    "                        if d == 2:\n",
    "                            fv += np.diff(endog[start - 1:end + 1])\n",
    "            else:\n",
    "                #IFF we need to use pre-sample values assume pre-sample\n",
    "                # residuals are zero, do this by a hack\n",
    "                if start == self.k_ar + self.k_diff or start is None:\n",
    "                    # do the first k_diff+1 separately\n",
    "                    p = self.k_ar\n",
    "                    q = self.k_ma\n",
    "                    k_exog = self.k_exog\n",
    "                    k_trend = self.k_trend\n",
    "                    k_diff = self.k_diff\n",
    "                    (trendparam, exparams,\n",
    "                     arparams, maparams) = _unpack_params(params, (p, q),\n",
    "                                                          k_trend,\n",
    "                                                          k_exog,\n",
    "                                                          reverse=True)\n",
    "                    # this is the hack\n",
    "                    self.k_ma = 0\n",
    " \n",
    "                    predict = super(ARIMA, self).predict(params, start, end,\n",
    "                                                         exog, dynamic)\n",
    "                    if not start:\n",
    "                        start = self._get_predict_start(start, dynamic)\n",
    "                        start += k_diff\n",
    "                    self.k_ma = q\n",
    "                    return endog[start-1] + np.cumsum(predict)\n",
    "                else:\n",
    "                    predict = super(ARIMA, self).predict(params, start, end,\n",
    "                                                         exog, dynamic)\n",
    "                    return endog[start-1] + np.cumsum(predict)\n",
    "            return fv\n",
    " \n",
    "        else:  # pragma : no cover\n",
    "            raise ValueError(\"typ %s not understood\" % typ)\n",
    " \n",
    "    predict.__doc__ = _arima_predict\n",
    " \n",
    " \n",
    "class ARMAResults(tsbase.TimeSeriesModelResults):\n",
    "    \"\"\"\n",
    "    Class to hold results from fitting an ARMA model.\n",
    " \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : ARMA instance\n",
    "        The fitted model instance\n",
    "    params : array\n",
    "        Fitted parameters\n",
    "    normalized_cov_params : array, optional\n",
    "        The normalized variance covariance matrix\n",
    "    scale : float, optional\n",
    "        Optional argument to scale the variance covariance matrix.\n",
    " \n",
    "    Returns\n",
    "    --------\n",
    "    **Attributes**\n",
    " \n",
    "    aic : float\n",
    "        Akaike Information Criterion\n",
    "        :math:`-2*llf+2* df_model`\n",
    "        where `df_model` includes all AR parameters, MA parameters, constant\n",
    "        terms parameters on constant terms and the variance.\n",
    "    arparams : array\n",
    "        The parameters associated with the AR coefficients in the model.\n",
    "    arroots : array\n",
    "        The roots of the AR coefficients are the solution to\n",
    "        (1 - arparams[0]*z - arparams[1]*z**2 -...- arparams[p-1]*z**k_ar) = 0\n",
    "        Stability requires that the roots in modulus lie outside the unit\n",
    "        circle.\n",
    "    bic : float\n",
    "        Bayes Information Criterion\n",
    "        -2*llf + log(nobs)*df_model\n",
    "        Where if the model is fit using conditional sum of squares, the\n",
    "        number of observations `nobs` does not include the `p` pre-sample\n",
    "        observations.\n",
    "    bse : array\n",
    "        The standard errors of the parameters. These are computed using the\n",
    "        numerical Hessian.\n",
    "    df_model : array\n",
    "        The model degrees of freedom = `k_exog` + `k_trend` + `k_ar` + `k_ma`\n",
    "    df_resid : array\n",
    "        The residual degrees of freedom = `nobs` - `df_model`\n",
    "    fittedvalues : array\n",
    "        The predicted values of the model.\n",
    "    hqic : float\n",
    "        Hannan-Quinn Information Criterion\n",
    "        -2*llf + 2*(`df_model`)*log(log(nobs))\n",
    "        Like `bic` if the model is fit using conditional sum of squares then\n",
    "        the `k_ar` pre-sample observations are not counted in `nobs`.\n",
    "    k_ar : int\n",
    "        The number of AR coefficients in the model.\n",
    "    k_exog : int\n",
    "        The number of exogenous variables included in the model. Does not\n",
    "        include the constant.\n",
    "    k_ma : int\n",
    "        The number of MA coefficients.\n",
    "    k_trend : int\n",
    "        This is 0 for no constant or 1 if a constant is included.\n",
    "    llf : float\n",
    "        The value of the log-likelihood function evaluated at `params`.\n",
    "    maparams : array\n",
    "        The value of the moving average coefficients.\n",
    "    maroots : array\n",
    "        The roots of the MA coefficients are the solution to\n",
    "        (1 + maparams[0]*z + maparams[1]*z**2 + ... + maparams[q-1]*z**q) = 0\n",
    "        Stability requires that the roots in modules lie outside the unit\n",
    "        circle.\n",
    "    model : ARMA instance\n",
    "        A reference to the model that was fit.\n",
    "    nobs : float\n",
    "        The number of observations used to fit the model. If the model is fit\n",
    "        using exact maximum likelihood this is equal to the total number of\n",
    "        observations, `n_totobs`. If the model is fit using conditional\n",
    "        maximum likelihood this is equal to `n_totobs` - `k_ar`.\n",
    "    n_totobs : float\n",
    "        The total number of observations for `endog`. This includes all\n",
    "        observations, even pre-sample values if the model is fit using `css`.\n",
    "    params : array\n",
    "        The parameters of the model. The order of variables is the trend\n",
    "        coefficients and the `k_exog` exognous coefficients, then the\n",
    "        `k_ar` AR coefficients, and finally the `k_ma` MA coefficients.\n",
    "    pvalues : array\n",
    "        The p-values associated with the t-values of the coefficients. Note\n",
    "        that the coefficients are assumed to have a Student's T distribution.\n",
    "    resid : array\n",
    "        The model residuals. If the model is fit using 'mle' then the\n",
    "        residuals are created via the Kalman Filter. If the model is fit\n",
    "        using 'css' then the residuals are obtained via `scipy.signal.lfilter`\n",
    "        adjusted such that the first `k_ma` residuals are zero. These zero\n",
    "        residuals are not returned.\n",
    "    scale : float\n",
    "        This is currently set to 1.0 and not used by the model or its results.\n",
    "    sigma2 : float\n",
    "        The variance of the residuals. If the model is fit by 'css',\n",
    "        sigma2 = ssr/nobs, where ssr is the sum of squared residuals. If\n",
    "        the model is fit by 'mle', then sigma2 = 1/nobs * sum(v**2 / F)\n",
    "        where v is the one-step forecast error and F is the forecast error\n",
    "        variance. See `nobs` for the difference in definitions depending on the\n",
    "        fit.\n",
    "    \"\"\"\n",
    "    _cache = {}\n",
    " \n",
    "    #TODO: use this for docstring when we fix nobs issue\n",
    " \n",
    "    def __init__(self, model, params, normalized_cov_params=None, scale=1.):\n",
    "        super(ARMAResults, self).__init__(model, params, normalized_cov_params,\n",
    "                                          scale)\n",
    "        self.sigma2 = model.sigma2\n",
    "        nobs = model.nobs\n",
    "        self.nobs = nobs\n",
    "        k_exog = model.k_exog\n",
    "        self.k_exog = k_exog\n",
    "        k_trend = model.k_trend\n",
    "        self.k_trend = k_trend\n",
    "        k_ar = model.k_ar\n",
    "        self.k_ar = k_ar\n",
    "        self.n_totobs = len(model.endog)\n",
    "        k_ma = model.k_ma\n",
    "        self.k_ma = k_ma\n",
    "        df_model = k_exog + k_trend + k_ar + k_ma\n",
    "        self._ic_df_model = df_model + 1\n",
    "        self.df_model = df_model\n",
    "        self.df_resid = self.nobs - df_model\n",
    "        self._cache = resettable_cache()\n",
    "        self.constant = 0  #Added by me\n",
    " \n",
    "    @cache_readonly\n",
    "    def arroots(self):\n",
    "        return np.roots(np.r_[1, -self.arparams])**-1\n",
    " \n",
    "    @cache_readonly\n",
    "    def maroots(self):\n",
    "        return np.roots(np.r_[1, self.maparams])**-1\n",
    " \n",
    "    @cache_readonly\n",
    "    def arfreq(self):\n",
    "        r\"\"\"\n",
    "        Returns the frequency of the AR roots.\n",
    " \n",
    "        This is the solution, x, to z = abs(z)*exp(2j*np.pi*x) where z are the\n",
    "        roots.\n",
    "        \"\"\"\n",
    "        z = self.arroots\n",
    "        if not z.size:\n",
    "            return\n",
    "        return np.arctan2(z.imag, z.real) / (2*pi)\n",
    " \n",
    "    @cache_readonly\n",
    "    def mafreq(self):\n",
    "        r\"\"\"\n",
    "        Returns the frequency of the MA roots.\n",
    " \n",
    "        This is the solution, x, to z = abs(z)*exp(2j*np.pi*x) where z are the\n",
    "        roots.\n",
    "        \"\"\"\n",
    "        z = self.maroots\n",
    "        if not z.size:\n",
    "            return\n",
    "        return np.arctan2(z.imag, z.real) / (2*pi)\n",
    " \n",
    "    @cache_readonly\n",
    "    def arparams(self):\n",
    "        k = self.k_exog + self.k_trend\n",
    "        return self.params[k:k+self.k_ar]\n",
    " \n",
    "    @cache_readonly\n",
    "    def maparams(self):\n",
    "        k = self.k_exog + self.k_trend\n",
    "        k_ar = self.k_ar\n",
    "        return self.params[k+k_ar:]\n",
    " \n",
    "    @cache_readonly\n",
    "    def llf(self):\n",
    "        return self.model.loglike(self.params)\n",
    " \n",
    "    @cache_readonly\n",
    "    def bse(self):\n",
    "        params = self.params\n",
    "        hess = self.model.hessian(params)\n",
    "        if len(params) == 1:  # can't take an inverse, ensure 1d\n",
    "            return np.sqrt(-1./hess[0])\n",
    "        return np.sqrt(np.diag(-inv(hess)))\n",
    " \n",
    "    def cov_params(self):  # add scale argument?\n",
    "        params = self.params\n",
    "        hess = self.model.hessian(params)\n",
    "        return -inv(hess)\n",
    " \n",
    "    @cache_readonly\n",
    "    def aic(self):\n",
    "        return -2 * self.llf + 2 * self._ic_df_model\n",
    " \n",
    "    @cache_readonly\n",
    "    def bic(self):\n",
    "        nobs = self.nobs\n",
    "        return -2 * self.llf + np.log(nobs) * self._ic_df_model\n",
    " \n",
    "    @cache_readonly\n",
    "    def hqic(self):\n",
    "        nobs = self.nobs\n",
    "        return -2 * self.llf + 2 * np.log(np.log(nobs)) * self._ic_df_model\n",
    " \n",
    "    @cache_readonly\n",
    "    def fittedvalues(self):\n",
    "        model = self.model\n",
    "        endog = model.endog.copy()\n",
    "        k_ar = self.k_ar\n",
    "        exog = model.exog  # this is a copy\n",
    "        if exog is not None:\n",
    "            if model.method == \"css\" and k_ar > 0:\n",
    "                exog = exog[k_ar:]\n",
    "        if model.method == \"css\" and k_ar > 0:\n",
    "            endog = endog[k_ar:]\n",
    "        fv = endog - self.resid\n",
    "        # add deterministic part back in\n",
    "        #k = self.k_exog + self.k_trend\n",
    "        #TODO: this needs to be commented out for MLE with constant\n",
    "        #if k != 0:\n",
    "        #    fv += dot(exog, self.params[:k])\n",
    "        return fv\n",
    " \n",
    "    @cache_readonly\n",
    "    def resid(self):\n",
    "        return self.model.geterrors(self.params)\n",
    " \n",
    "    @cache_readonly\n",
    "    def pvalues(self):\n",
    "    #TODO: same for conditional and unconditional?\n",
    "        df_resid = self.df_resid\n",
    "        return t.sf(np.abs(self.tvalues), df_resid) * 2\n",
    " \n",
    "    def predict(self, start=None, end=None, exog=None, dynamic=False):\n",
    "        return self.model.predict(self.params, start, end, exog, dynamic)\n",
    "    predict.__doc__ = _arma_results_predict\n",
    " \n",
    "    def _forecast_error(self, steps):\n",
    "        sigma2 = self.sigma2\n",
    "        ma_rep = arma2ma(np.r_[1, -self.arparams],\n",
    "                         np.r_[1, self.maparams], nobs=steps)\n",
    " \n",
    "        fcasterr = np.sqrt(sigma2 * np.cumsum(ma_rep**2))\n",
    "        return fcasterr\n",
    " \n",
    "    def _forecast_conf_int(self, forecast, fcasterr, alpha):\n",
    "        const = norm.ppf(1 - alpha / 2.)\n",
    "        conf_int = np.c_[forecast - const * fcasterr,\n",
    "                         forecast + const * fcasterr]\n",
    " \n",
    "        return conf_int\n",
    " \n",
    "    def forecast(self, steps=1, exog=None, alpha=.05):\n",
    "        \"\"\"\n",
    "        Out-of-sample forecasts\n",
    " \n",
    "        Parameters\n",
    "        ----------\n",
    "        steps : int\n",
    "            The number of out of sample forecasts from the end of the\n",
    "            sample.\n",
    "        exog : array\n",
    "            If the model is an ARMAX, you must provide out of sample\n",
    "            values for the exogenous variables. This should not include\n",
    "            the constant.\n",
    "        alpha : float\n",
    "            The confidence intervals for the forecasts are (1 - alpha) %\n",
    " \n",
    "        Returns\n",
    "        -------\n",
    "        forecast : array\n",
    "            Array of out of sample forecasts\n",
    "        stderr : array\n",
    "            Array of the standard error of the forecasts.\n",
    "        conf_int : array\n",
    "            2d array of the confidence interval for the forecast\n",
    "        \"\"\"\n",
    "        if exog is not None:\n",
    "            #TODO: make a convenience function for this. we're using the\n",
    "            # pattern elsewhere in the codebase\n",
    "            exog = np.asarray(exog)\n",
    "            if self.k_exog == 1 and exog.ndim == 1:\n",
    "                exog = exog[:, None]\n",
    "            elif exog.ndim == 1:\n",
    "                if len(exog) != self.k_exog:\n",
    "                    raise ValueError(\"1d exog given and len(exog) != k_exog\")\n",
    "                exog = exog[None, :]\n",
    "            if exog.shape[0] != steps:\n",
    "                raise ValueError(\"new exog needed for each step\")\n",
    "            # prepend in-sample exog observations\n",
    "            exog = np.vstack((self.model.exog[-self.k_ar:, self.k_trend:],\n",
    "                              exog))\n",
    " \n",
    "        forecast, ct = _arma_predict_out_of_sample(self.params,\n",
    "                                               steps, self.resid, self.k_ar,\n",
    "                                               self.k_ma, self.k_trend,\n",
    "                                               self.k_exog, self.model.endog,\n",
    "                                               exog, method=self.model.method)\n",
    "        self.constant = ct\n",
    " \n",
    "        # compute the standard errors\n",
    "        fcasterr = self._forecast_error(steps)\n",
    "        conf_int = self._forecast_conf_int(forecast, fcasterr, alpha)\n",
    " \n",
    "        return forecast, fcasterr, conf_int\n",
    " \n",
    "    def summary(self, alpha=.05):\n",
    "        \"\"\"Summarize the Model\n",
    " \n",
    "        Parameters\n",
    "        ----------\n",
    "        alpha : float, optional\n",
    "            Significance level for the confidence intervals.\n",
    " \n",
    "        Returns\n",
    "        -------\n",
    "        smry : Summary instance\n",
    "            This holds the summary table and text, which can be printed or\n",
    "            converted to various output formats.\n",
    " \n",
    "        See Also\n",
    "        --------\n",
    "        statsmodels.iolib.summary.Summary\n",
    "        \"\"\"\n",
    "        from statsmodels.iolib.summary import Summary\n",
    "        model = self.model\n",
    "        title = model.__class__.__name__ + ' Model Results'\n",
    "        method = model.method\n",
    "        # get sample TODO: make better sample machinery for estimation\n",
    "        k_diff = getattr(self, 'k_diff', 0)\n",
    "        if 'mle' in method:\n",
    "            start = k_diff\n",
    "        else:\n",
    "            start = k_diff + self.k_ar\n",
    "        if self.data.dates is not None:\n",
    "            dates = self.data.dates\n",
    "            sample = [dates[start].strftime('%m-%d-%Y')]\n",
    "            sample += ['- ' + dates[-1].strftime('%m-%d-%Y')]\n",
    "        else:\n",
    "            sample = str(start) + ' - ' + str(len(self.data.orig_endog))\n",
    " \n",
    "        k_ar, k_ma = self.k_ar, self.k_ma\n",
    "        if not k_diff:\n",
    "            order = str((k_ar, k_ma))\n",
    "        else:\n",
    "            order = str((k_ar, k_diff, k_ma))\n",
    "        top_left = [('Dep. Variable:', None),\n",
    "                    ('Model:', [model.__class__.__name__ + order]),\n",
    "                    ('Method:', [method]),\n",
    "                    ('Date:', None),\n",
    "                    ('Time:', None),\n",
    "                    ('Sample:', [sample[0]]),\n",
    "                    ('', [sample[1]])\n",
    "                    ]\n",
    " \n",
    "        top_right = [\n",
    "                     ('No. Observations:', [str(len(self.model.endog))]),\n",
    "                     ('Log Likelihood', [\"%#5.3f\" % self.llf]),\n",
    "                     ('S.D. of innovations', [\"%#5.3f\" % self.sigma2**.5]),\n",
    "                     ('AIC', [\"%#5.3f\" % self.aic]),\n",
    "                     ('BIC', [\"%#5.3f\" % self.bic]),\n",
    "                     ('HQIC', [\"%#5.3f\" % self.hqic])]\n",
    " \n",
    "        smry = Summary()\n",
    "        smry.add_table_2cols(self, gleft=top_left, gright=top_right,\n",
    "                             title=title)\n",
    "        smry.add_table_params(self, alpha=alpha, use_t=False)\n",
    " \n",
    "        # Make the roots table\n",
    "        from statsmodels.iolib.table import SimpleTable\n",
    " \n",
    "        if k_ma and k_ar:\n",
    "            arstubs = [\"AR.%d\" % i for i in range(1, k_ar + 1)]\n",
    "            mastubs = [\"MA.%d\" % i for i in range(1, k_ma + 1)]\n",
    "            stubs = arstubs + mastubs\n",
    "            roots = np.r_[self.arroots, self.maroots]\n",
    "            freq = np.r_[self.arfreq, self.mafreq]\n",
    "        elif k_ma:\n",
    "            mastubs = [\"MA.%d\" % i for i in range(1, k_ma + 1)]\n",
    "            stubs = mastubs\n",
    "            roots = self.maroots\n",
    "            freq = self.mafreq\n",
    "        elif k_ar:\n",
    "            arstubs = [\"AR.%d\" % i for i in range(1, k_ar + 1)]\n",
    "            stubs = arstubs\n",
    "            roots = self.arroots\n",
    "            freq = self.arfreq\n",
    "        else:  # 0,0 model\n",
    "            stubs = []\n",
    "        if len(stubs):  # not 0, 0\n",
    "            modulus = np.abs(roots)\n",
    "            data = np.column_stack((roots.real, roots.imag, modulus, freq))\n",
    "            roots_table = SimpleTable(data,\n",
    "                                      headers=['           Real',\n",
    "                                               '         Imaginary',\n",
    "                                               '         Modulus',\n",
    "                                               '        Frequency'],\n",
    "                                      title=\"Roots\",\n",
    "                                      stubs=stubs,\n",
    "                                      data_fmts=[\"%17.4f\", \"%+17.4fj\",\n",
    "                                                 \"%17.4f\", \"%17.4f\"])\n",
    " \n",
    "            smry.tables.append(roots_table)\n",
    "        return smry\n",
    " \n",
    "    def summary2(self, title=None, alpha=.05, float_format=\"%.4f\"):\n",
    "        \"\"\"Experimental summary function for ARIMA Results\n",
    " \n",
    "        Parameters\n",
    "        -----------\n",
    "        title : string, optional\n",
    "            Title for the top table. If not None, then this replaces the\n",
    "            default title\n",
    "        alpha : float\n",
    "            significance level for the confidence intervals\n",
    "        float_format: string\n",
    "            print format for floats in parameters summary\n",
    " \n",
    "        Returns\n",
    "        -------\n",
    "        smry : Summary instance\n",
    "            This holds the summary table and text, which can be printed or\n",
    "            converted to various output formats.\n",
    " \n",
    "        See Also\n",
    "        --------\n",
    "        statsmodels.iolib.summary2.Summary : class to hold summary\n",
    "            results\n",
    " \n",
    "        \"\"\"\n",
    "        from pandas import DataFrame\n",
    "        # get sample TODO: make better sample machinery for estimation\n",
    "        k_diff = getattr(self, 'k_diff', 0)\n",
    "        if 'mle' in self.model.method:\n",
    "            start = k_diff\n",
    "        else:\n",
    "            start = k_diff + self.k_ar\n",
    "        if self.data.dates is not None:\n",
    "            dates = self.data.dates\n",
    "            sample = [dates[start].strftime('%m-%d-%Y')]\n",
    "            sample += [dates[-1].strftime('%m-%d-%Y')]\n",
    "        else:\n",
    "            sample = str(start) + ' - ' + str(len(self.data.orig_endog))\n",
    " \n",
    "        k_ar, k_ma = self.k_ar, self.k_ma\n",
    " \n",
    "        # Roots table\n",
    "        if k_ma and k_ar:\n",
    "            arstubs = [\"AR.%d\" % i for i in range(1, k_ar + 1)]\n",
    "            mastubs = [\"MA.%d\" % i for i in range(1, k_ma + 1)]\n",
    "            stubs = arstubs + mastubs\n",
    "            roots = np.r_[self.arroots, self.maroots]\n",
    "            freq = np.r_[self.arfreq, self.mafreq]\n",
    "        elif k_ma:\n",
    "            mastubs = [\"MA.%d\" % i for i in range(1, k_ma + 1)]\n",
    "            stubs = mastubs\n",
    "            roots = self.maroots\n",
    "            freq = self.mafreq\n",
    "        elif k_ar:\n",
    "            arstubs = [\"AR.%d\" % i for i in range(1, k_ar + 1)]\n",
    "            stubs = arstubs\n",
    "            roots = self.arroots\n",
    "            freq = self.arfreq\n",
    "        else:  # 0, 0 order\n",
    "            stubs = []\n",
    " \n",
    "        if len(stubs):\n",
    "            modulus = np.abs(roots)\n",
    "            data = np.column_stack((roots.real, roots.imag, modulus, freq))\n",
    "            data = DataFrame(data)\n",
    "            data.columns = ['Real', 'Imaginary', 'Modulus', 'Frequency']\n",
    "            data.index = stubs\n",
    " \n",
    "        # Summary\n",
    "        from statsmodels.iolib import summary2\n",
    "        smry = summary2.Summary()\n",
    " \n",
    "        # Model info\n",
    "        model_info = summary2.summary_model(self)\n",
    "        model_info['Method:'] = self.model.method\n",
    "        model_info['Sample:'] = sample[0]\n",
    "        model_info['   '] = sample[-1]\n",
    "        model_info['S.D. of innovations:'] = \"%#5.3f\" % self.sigma2**.5\n",
    "        model_info['HQIC:'] = \"%#5.3f\" % self.hqic\n",
    "        model_info['No. Observations:'] = str(len(self.model.endog))\n",
    " \n",
    "        # Parameters\n",
    "        params = summary2.summary_params(self)\n",
    "        smry.add_dict(model_info)\n",
    "        smry.add_df(params, float_format=float_format)\n",
    "        if len(stubs):\n",
    "            smry.add_df(data, float_format=\"%17.4f\")\n",
    "        smry.add_title(results=self, title=title)\n",
    " \n",
    "        return smry\n",
    " \n",
    "    def plot_predict(self, start=None, end=None, exog=None, dynamic=False,\n",
    "                     alpha=.05, plot_insample=True, ax=None):\n",
    "        from statsmodels.graphics.utils import _import_mpl, create_mpl_ax\n",
    "        _ = _import_mpl()\n",
    "        fig, ax = create_mpl_ax(ax)\n",
    " \n",
    " \n",
    "        # use predict so you set dates\n",
    "        forecast = self.predict(start, end, exog, dynamic)\n",
    "        # doing this twice. just add a plot keyword to predict?\n",
    "        start = self.model._get_predict_start(start, dynamic=False)\n",
    "        end, out_of_sample = self.model._get_predict_end(end, dynamic=False)\n",
    " \n",
    "        if out_of_sample:\n",
    "            steps = out_of_sample\n",
    "            fc_error = self._forecast_error(steps)\n",
    "            conf_int = self._forecast_conf_int(forecast[-steps:], fc_error,\n",
    "                                               alpha)\n",
    " \n",
    " \n",
    "        if hasattr(self.data, \"predict_dates\"):\n",
    "            from pandas import TimeSeries\n",
    "            forecast = TimeSeries(forecast, index=self.data.predict_dates)\n",
    "            ax = forecast.plot(ax=ax, label='forecast')\n",
    "        else:\n",
    "            ax.plot(forecast)\n",
    " \n",
    "        x = ax.get_lines()[-1].get_xdata()\n",
    "        if out_of_sample:\n",
    "            label = \"{0:.0%} confidence interval\".format(1 - alpha)\n",
    "            ax.fill_between(x[-out_of_sample:], conf_int[:, 0], conf_int[:, 1],\n",
    "                            color='gray', alpha=.5, label=label)\n",
    " \n",
    "        if plot_insample:\n",
    "            ax.plot(x[:end + 1 - start], self.model.endog[start:end+1],\n",
    "                    label=self.model.endog_names)\n",
    " \n",
    "        ax.legend(loc='best')\n",
    " \n",
    "        return fig\n",
    "    plot_predict.__doc__ = _plot_predict\n",
    " \n",
    " \n",
    "class ARMAResultsWrapper(wrap.ResultsWrapper):\n",
    "    _attrs = {}\n",
    "    _wrap_attrs = wrap.union_dicts(tsbase.TimeSeriesResultsWrapper._wrap_attrs,\n",
    "                                   _attrs)\n",
    "    _methods = {}\n",
    "    _wrap_methods = wrap.union_dicts(tsbase.TimeSeriesResultsWrapper._wrap_methods,\n",
    "                                     _methods)\n",
    "wrap.populate_wrapper(ARMAResultsWrapper, ARMAResults)\n",
    " \n",
    " \n",
    "class ARIMAResults(ARMAResults):\n",
    "    def predict(self, start=None, end=None, exog=None, typ='linear',\n",
    "                dynamic=False):\n",
    "        return self.model.predict(self.params, start, end, exog, typ, dynamic)\n",
    "    predict.__doc__ = _arima_results_predict\n",
    " \n",
    "    def _forecast_error(self, steps):\n",
    "        sigma2 = self.sigma2\n",
    "        ma_rep = arma2ma(np.r_[1, -self.arparams],\n",
    "                         np.r_[1, self.maparams], nobs=steps)\n",
    " \n",
    "        fcerr = np.sqrt(np.cumsum(cumsum_n(ma_rep, self.k_diff)**2)*sigma2)\n",
    "        return fcerr\n",
    " \n",
    "    def _forecast_conf_int(self, forecast, fcerr, alpha):\n",
    "        const = norm.ppf(1 - alpha/2.)\n",
    "        conf_int = np.c_[forecast - const*fcerr, forecast + const*fcerr]\n",
    "        return conf_int\n",
    " \n",
    "    def forecast(self, steps=1, exog=None, alpha=.05):\n",
    "        \"\"\"\n",
    "        Out-of-sample forecasts\n",
    " \n",
    "        Parameters\n",
    "        ----------\n",
    "        steps : int\n",
    "            The number of out of sample forecasts from the end of the\n",
    "            sample.\n",
    "        exog : array\n",
    "            If the model is an ARIMAX, you must provide out of sample\n",
    "            values for the exogenous variables. This should not include\n",
    "            the constant.\n",
    "        alpha : float\n",
    "            The confidence intervals for the forecasts are (1 - alpha) %\n",
    " \n",
    "        Returns\n",
    "        -------\n",
    "        forecast : array\n",
    "            Array of out of sample forecasts\n",
    "        stderr : array\n",
    "            Array of the standard error of the forecasts.\n",
    "        conf_int : array\n",
    "            2d array of the confidence interval for the forecast\n",
    " \n",
    "        Notes\n",
    "        -----\n",
    "        Prediction is done in the levels of the original endogenous variable.\n",
    "        If you would like prediction of differences in levels use `predict`.\n",
    "        \"\"\"\n",
    "        if exog is not None:\n",
    "            if self.k_exog == 1 and exog.ndim == 1:\n",
    "                exog = exog[:, None]\n",
    "            if exog.shape[0] != steps:\n",
    "                raise ValueError(\"new exog needed for each step\")\n",
    "            # prepend in-sample exog observations\n",
    "            exog = np.vstack((self.model.exog[-self.k_ar:, self.k_trend:],\n",
    "                              exog))\n",
    "        forecast, ct = _arma_predict_out_of_sample(self.params, steps, self.resid,\n",
    "                                               self.k_ar, self.k_ma,\n",
    "                                               self.k_trend, self.k_exog,\n",
    "                                               self.model.endog,\n",
    "                                               exog, method=self.model.method)\n",
    " \n",
    "        #self.constant = ct\n",
    "        d = self.k_diff\n",
    "        endog = self.model.data.endog[-d:]\n",
    "        forecast = unintegrate(forecast, unintegrate_levels(endog, d))[d:]\n",
    " \n",
    "        # get forecast errors\n",
    "        fcerr = self._forecast_error(steps)\n",
    "        conf_int = self._forecast_conf_int(forecast, fcerr, alpha)\n",
    "        return forecast, fcerr, conf_int\n",
    " \n",
    "    def plot_predict(self, start=None, end=None, exog=None, dynamic=False,\n",
    "                     alpha=.05, plot_insample=True, ax=None):\n",
    "        from statsmodels.graphics.utils import _import_mpl, create_mpl_ax\n",
    "        _ = _import_mpl()\n",
    "        fig, ax = create_mpl_ax(ax)\n",
    " \n",
    "        # use predict so you set dates\n",
    "        forecast = self.predict(start, end, exog, 'levels', dynamic)\n",
    "        # doing this twice. just add a plot keyword to predict?\n",
    "        start = self.model._get_predict_start(start, dynamic=dynamic)\n",
    "        end, out_of_sample = self.model._get_predict_end(end, dynamic=dynamic)\n",
    " \n",
    "        if out_of_sample:\n",
    "            steps = out_of_sample\n",
    "            fc_error = self._forecast_error(steps)\n",
    "            conf_int = self._forecast_conf_int(forecast[-steps:], fc_error,\n",
    "                                               alpha)\n",
    " \n",
    "        if hasattr(self.data, \"predict_dates\"):\n",
    "            from pandas import TimeSeries\n",
    "            forecast = TimeSeries(forecast, index=self.data.predict_dates)\n",
    "            ax = forecast.plot(ax=ax, label='forecast')\n",
    "        else:\n",
    "            ax.plot(forecast)\n",
    " \n",
    "        x = ax.get_lines()[-1].get_xdata()\n",
    "        if out_of_sample:\n",
    "            label = \"{0:.0%} confidence interval\".format(1 - alpha)\n",
    "            ax.fill_between(x[-out_of_sample:], conf_int[:, 0], conf_int[:, 1],\n",
    "                            color='gray', alpha=.5, label=label)\n",
    " \n",
    "        if plot_insample:\n",
    "            import re\n",
    "            k_diff = self.k_diff\n",
    "            label = re.sub(\"D\\d*\\.\", \"\", self.model.endog_names)\n",
    "            levels = unintegrate(self.model.endog,\n",
    "                                 self.model._first_unintegrate)\n",
    "            ax.plot(x[:end + 1 - start],\n",
    "                    levels[start + k_diff:end + k_diff + 1], label=label)\n",
    " \n",
    "        ax.legend(loc='best')\n",
    " \n",
    "        return fig\n",
    " \n",
    "    plot_predict.__doc__ = _arima_plot_predict\n",
    " \n",
    " \n",
    "class ARIMAResultsWrapper(ARMAResultsWrapper):\n",
    "    pass\n",
    "wrap.populate_wrapper(ARIMAResultsWrapper, ARIMAResults)\n",
    " \n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    import statsmodels.api as sm\n",
    " \n",
    "    # simulate arma process\n",
    "    from statsmodels.tsa.arima_process import arma_generate_sample\n",
    "    y = arma_generate_sample([1., -.75], [1., .25], nsample=1000)\n",
    "    arma = ARMA(y)\n",
    "    res = arma.fit(trend='nc', order=(1, 1))\n",
    " \n",
    "    np.random.seed(12345)\n",
    "    y_arma22 = arma_generate_sample([1., -.85, .35], [1, .25, -.9],\n",
    "                                    nsample=1000)\n",
    "    arma22 = ARMA(y_arma22)\n",
    "    res22 = arma22.fit(trend='nc', order=(2, 2))\n",
    " \n",
    "    # test CSS\n",
    "    arma22_css = ARMA(y_arma22)\n",
    "    res22css = arma22_css.fit(trend='nc', order=(2, 2), method='css')\n",
    " \n",
    "    data = sm.datasets.sunspots.load()\n",
    "    ar = ARMA(data.endog)\n",
    "    resar = ar.fit(trend='nc', order=(9, 0))\n",
    " \n",
    "    y_arma31 = arma_generate_sample([1, -.75, -.35, .25], [.1],\n",
    "                                    nsample=1000)\n",
    " \n",
    "    arma31css = ARMA(y_arma31)\n",
    "    res31css = arma31css.fit(order=(3, 1), method=\"css\", trend=\"nc\",\n",
    "                             transparams=True)\n",
    " \n",
    "    y_arma13 = arma_generate_sample([1., -.75], [1, .25, -.5, .8],\n",
    "                                    nsample=1000)\n",
    "    arma13css = ARMA(y_arma13)\n",
    "    res13css = arma13css.fit(order=(1, 3), method='css', trend='nc')\n",
    " \n",
    "# check css for p < q and q < p\n",
    "    y_arma41 = arma_generate_sample([1., -.75, .35, .25, -.3], [1, -.35],\n",
    "                                    nsample=1000)\n",
    "    arma41css = ARMA(y_arma41)\n",
    "    res41css = arma41css.fit(order=(4, 1), trend='nc', method='css')\n",
    " \n",
    "    y_arma14 = arma_generate_sample([1, -.25], [1., -.75, .35, .25, -.3],\n",
    "                                    nsample=1000)\n",
    "    arma14css = ARMA(y_arma14)\n",
    "    res14css = arma14css.fit(order=(4, 1), trend='nc', method='css')\n",
    " \n",
    "    # ARIMA Model\n",
    "    from statsmodels.datasets import webuse\n",
    "    dta = webuse('wpi1')\n",
    "    wpi = dta['wpi']\n",
    " \n",
    "    mod = ARIMA(wpi, (1, 1, 1)).fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "import sys\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "class arima_model:\n",
    " \n",
    "    def __init__(self, ts, maxLag=9):\n",
    "        self.data_ts = ts\n",
    "        self.resid_ts = None\n",
    "        self.predict_ts = None\n",
    "        self.maxLag = maxLag\n",
    "        self.p = maxLag\n",
    "        self.q = maxLag\n",
    "        self.properModel = None\n",
    "        self.bic = sys.maxsize\n",
    " \n",
    "    #    ARIMA      \n",
    "    def get_proper_model(self,pp=None,qq=None):\n",
    "        self._proper_model(pp,qq)\n",
    "        self.predict_ts = deepcopy(self.properModel.predict())\n",
    "        self.resid_ts = deepcopy(self.properModel.resid)\n",
    " \n",
    "    #  p, q       arima,      ,     0\n",
    "    def _proper_model(self,pp,qq):\n",
    "        for p in [pp] if not(pp is None) else np.arange(self.maxLag):\n",
    "            for q in [qq] if not(qq is None) else np.arange(self.maxLag):\n",
    "                # print p,q,self.bic\n",
    "                model = ARMA(self.data_ts, order=(p, q))\n",
    "                try:\n",
    "                    results_ARMA = model.fit(disp=-1, method='css')\n",
    "                except:\n",
    "                    continue\n",
    "                bic = results_ARMA.bic\n",
    "                # print 'bic:',bic,'self.bic:',self.bic\n",
    "                if bic < self.bic:\n",
    "                    self.p = p\n",
    "                    self.q = q\n",
    "                    self.properModel = results_ARMA\n",
    "                    self.bic = bic\n",
    "                    self.resid_ts = deepcopy(self.properModel.resid)\n",
    "                    self.predict_ts = self.properModel.predict()\n",
    " \n",
    "    #   \n",
    "    def certain_model(self, p, q):\n",
    "            model = ARMA(self.data_ts, order=(p, q))\n",
    "            try:\n",
    "                self.properModel = model.fit( disp=-1, method='css')\n",
    "                self.p = p\n",
    "                self.q = q\n",
    "                self.bic = self.properModel.bic\n",
    "                self.predict_ts = self.properModel.predict()\n",
    "                self.resid_ts = deepcopy(self.properModel.resid)\n",
    "            except:\n",
    "                print ('You can not fit the model with this parameter p,q, ' \\\n",
    "                      'please use the get_proper_model method to get the best model')\n",
    " \n",
    "    #     \n",
    "    def forecast_next_day_value(self, type='day'):\n",
    "        #     arima_model   statsmodels    ,          \n",
    "        self.properModel.forecast()\n",
    "        if self.data_ts.index[-1] != self.resid_ts.index[-1]:\n",
    "            raise ValueError('''The index is different in data_ts and resid_ts, please add new data to data_ts.\n",
    "            If you just want to forecast the next day data without add the real next day data to data_ts,\n",
    "            please run the predict method which arima_model included itself''')\n",
    "        if not self.properModel:\n",
    "            raise ValueError('The arima model have not computed, please run the proper_model method before')\n",
    "        para = self.properModel.params\n",
    " \n",
    "        # print self.properModel.params\n",
    "        if self.p == 0:   # It will get all the value series with setting self.data_ts[-self.p:] when p is zero\n",
    "            ma_value = self.resid_ts[-self.q:]\n",
    "            values = ma_value.reindex(index=ma_value.index[::-1])\n",
    "        elif self.q == 0:\n",
    "            ar_value = self.data_ts[-self.p:]\n",
    "            values = ar_value.reindex(index=ar_value.index[::-1])\n",
    "        else:\n",
    "            ar_value = self.data_ts[-self.p:]\n",
    "            ar_value = ar_value.reindex(index=ar_value.index[::-1])\n",
    "            ma_value = self.resid_ts[-self.q:]\n",
    "            ma_value = ma_value.reindex(index=ma_value.index[::-1])\n",
    "            values = ar_value.append(ma_value)\n",
    " \n",
    "        predict_value = np.dot(para[1:], values) #+ self.properModel.constant[0]\n",
    "        self._add_new_data(self.predict_ts, predict_value, type)\n",
    "        return predict_value\n",
    " \n",
    "    #    ,        .\n",
    "    def _add_new_data(self, ts, dat, type='day'):\n",
    "        if type == 'day':\n",
    "            new_index = ts.index[-1] + relativedelta(days=1)\n",
    "        elif type == 'month':\n",
    "            new_index = ts.index[-1] + relativedelta(months=1)\n",
    "        ts[new_index] = dat\n",
    " \n",
    "    def add_today_data(self, dat, type='day'):\n",
    "        self._add_new_data(self.data_ts, dat, type)\n",
    "        if self.data_ts.index[-1] != self.predict_ts.index[-1]:\n",
    "            raise ValueError('You must use the forecast_next_day_value method forecast the value of today before')\n",
    "        self._add_new_data(self.resid_ts, self.data_ts[-1] - self.predict_ts[-1], type)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bic: -1090.4420935807966 p: 0 q: 1\n",
      "[0.00084349]\n",
      "0.0008195425185956893\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABhb0lEQVR4nO2dd3yUVfb/3zOT3nshgQSSkEYChNCkgwiogCCKrgooimIv69fd1f3p6qrorm13bSirwbUiCiqCIkgTEEIPIZBAEtJI73XK/f3xJIGQNpPMhBDu+/XKK5l7n+ee+4zymTvnnnuOSgghkEgkEkmfQn2pJyCRSCQS8yPFXSKRSPogUtwlEomkDyLFXSKRSPogUtwlEomkDyLFXSKRSPogUtwlEomkDyLFXdKjBAcHY29vj5OTE35+fixZsoSqqqrm/iVLlqBSqVi/fn2L+x577DFUKhUff/wxAA0NDTzxxBMEBgbi5OREcHAwjz76aJt2mn4efPBBo+aYlJTEjBkz8PLyQqVSter/z3/+Q3x8PLa2tixZsqTDsYQQPPPMMwQEBODq6srkyZM5fvx4c39JSQkLFy7E09MTLy8vbrvtNioqKpr7//rXvxITE4OVlRXPPfdci7FfeumlFs9nb2+PWq2mqKjIqOeU9G2kuEt6nO+//56qqioOHz7MoUOHePnll1v0Dx48mNWrVze/1ul0fPXVV4SEhDS3vfzyyyQmJrJv3z4qKyvZtm0bcXFxbdpp+vnPf/5j1Pysra25+eabWbVqVZv9/fr145lnnuGuu+7qdKw1a9bw3//+l507d1JSUsLYsWO54447mvufeeYZSktLSU9P5/Tp0+Tn57cQ8dDQUF599VWuu+66VmP/5S9/afF8Tz31FJMnT8bLy8uo55T0baS4Sy4Zfn5+zJgxg8OHD7donz17Nrt27aK0tBSATZs2ERsbi5+fX/M1+/fvZ968efTr1w+VSkVwcDCLFi0yy7zCw8NZunQp0dHRbfbPnz+fG264AU9Pz07HSk9PZ/z48QwaNAiNRsPtt99OcnJyi/4bbrgBFxcXXF1dmTdvXouV/eLFi5k1axbOzs4d2hFCsHr1ahYvXmzkU0r6OlLcJZeM7OxsNm7cSGhoaIt2Ozs75s6dyxdffAHA6tWrWwn3mDFjeP3113nnnXc4duwYpmTROHv2LG5ubpw9e7b7D9EJt9xyC6dPn+bUqVNotVoSEhKYOXNmc/8DDzzADz/8QGlpKaWlpaxdu5ZZs2aZbGfnzp0UFBRw4403mnP6kssYKe6SHueGG27A2dmZ/v374+Pjw9/+9rdW1yxatIjVq1dTVlbG9u3bueGGG1r0//nPf+app57i008/JT4+noCAABISElrZcXNza/754IMPABgwYABlZWUMGDDAYs/YhL+/P+PHjyc8PBx7e3vWrFnDG2+80dwfFxdHQ0MDnp6eeHp6otFouP/++022k5CQwIIFC3BycjLn9CWXMVLcJT3OunXrmv3kKSkpbW4Ajh8/nsLCQl588UWuv/567O3tW/RrNBoeeOABfvvtN8rKynj66ae56667OHHiRAs7ZWVlzT/33HOPxZ/tYp5//nn2799PVlYWdXV1PPvss0ydOpWamhoAbr75ZgYPHkxlZSUVFRWEhIRw++23m2SjpqaGNWvWSJeMpAVS3CWXjEmTJrFkyRL++Mc/ttl/++2389prr3XqS7e3t+eBBx7A3d29hT+7N3D48GEWLlxIYGAgVlZWLFmyhNLS0uZ5Hj58mHvvvRdHR0ecnJy47777+PHHH02y8e233+Lh4cHkyZMt8ASSyxUp7pJLyqOPPsrmzZs5cuRIq76HH36YzZs3M3HixFZ9b775Jtu2baO2thadTkdCQgKVlZUMHz6823MSQlBXV0dDQwMAdXV11NfXN/frdDrq6urQ6/Xo9Xrq6urQ6XRtjjVy5EjWrFlDfn4+BoOBTz75BK1W27zPMHLkSD788ENqa2upra1l5cqVxMbGNt+v1Wqpq6vDYDC0sHshCQkJLFq0qM2wTckVjJBIepCgoCCxefPmFm333XefmD9/vhBCiMWLF4unn366zXvHjRsnPvroIyGEEO+//76Ii4sTLi4uwtXVVYwcOVJ8//33LezY2dkJR0fH5p8bbrhBCCFEZmamcHR0FJmZmW3aSU9PF0CLn6CgoOb+Z599tlX/s88+2+bYtbW14v777xd+fn7C2dlZDB8+XGzcuLF5rDNnzojrr79eeHh4CHd3dzFjxgxx6tSp5v7Fixe3stX0HgghRHZ2ttBoNCI1NbWDd11yJaISQhbrkEgkkr6GdMtIJBJJH0SKu0QikfRBpLhLJBJJH0SKu0QikfRBrC71BAC8vLwIDg6+1NOQSCSSy4qMjIx2s4D2CnEPDg4mMTHxUk9DIpFILivi4+Pb7ZNuGYlEIumDSHGXSCSSPogUd4lEIumD9Aqfe1totVqys7Opq6u71FPp1djZ2REYGIi1tfWlnopEIulF9Fpxz87OxtnZmeDgYJkQqR2EEBQXF5Odnc3AgQMv9XQkEkkvote6Zerq6vD09JTC3gEqlQpPT0/57UYikbSi14o7IIXdCOR7JJFI2qJXi7tEIpFcSeh0sGoVVFd3fywp7h2g0WgYNmwYQ4YM4aabbmoujdYVlixZwtdffw3A3Xff3WHFoG3btrF79+4u25JIJJcnCQlw991gYjGuNpHi3gH29vYcPnyYpKQkbGxseO+991r0t1d9pzM+/PBDoqKi2u2X4i6RXHnodPDyy8rfZWXdH0+Ku5FMmDCBtLQ0tm3bxoQJE5gzZw5RUVHo9XqefPJJRo4cSWxsLO+//z6gRLI8+OCDhIeHc/XVV1NQUNA81uTJk5vTLWzatIm4uDiGDh3KtGnTyMjI4L333uONN95g2LBh7Ny585I8r0Qi6Vm+/BJOn1b+rqjo/ni9NhTyQh59FA4fNu+Yw4bBm28ad61Op2Pjxo3MnDkTgIMHD5KUlMTAgQNZuXIlrq6u7N+/n/r6esaNG8c111zDoUOHOHnyJMnJyeTn5xMVFcVdd93VYtzCwkLuueceduzYwcCBAykpKcHDw4P77rsPJyendgtHSySSvoXBAC++CEOGQFLSFSTul4ra2lqGDRsGKCv3pUuXsnv3bkaNGtUcV/7zzz9z9OjRZn96eXk5qamp7Nixg1tvvRWNRkO/fv2YOnVqq/H37t3LxIkTm8fy8PDomQeTSCS9ioxZy/nziSpsvviEe+65gsTd2BW2uWnyuV+Mo6Nj899CCP79738zY8aMFtf8aI4dEYlEcmWwezd3cBT9sGd5wiXULOIufe7dZMaMGbz77rtotVoATp06RXV1NRMnTuTLL79Er9eTl5fHr7/+2ureMWPGsGPHDtLT0wEoKSkBwNnZmcrKyp57CIlEcklxrFNysmveexsXF/Os3KW4d5O7776bqKgo4uLiGDJkCPfeey86nY558+YRFhZGVFQUixYtYuzYsa3u9fb2ZuXKlcyfP5+hQ4eycOFCAGbPns23334rN1QlkisBIXDTNRbc+OgjfB2rzCLuKiGE6P4w3SM+Pr5VsY4TJ04QGRl5iWZ0eSHfK4nk8kVUVKJydeH44BuIPrWOtyLf43OXe9m7t/N729LOJuTKXSKRSC4hlenKqj172GwYPpx5Of+morz7a26jxL2srIwFCxYQERFBZGQke/bsoaSkhOnTpxMWFsb06dMpLS0FlA3Ghx9+mNDQUGJjYzl48GC3JymRSCR9lYozirhb9/OGBx9kQMVxwor2dHtco8T9kUceYebMmaSkpHDkyBEiIyNZsWIF06ZNIzU1lWnTprFixQoANm7cSGpqKqmpqaxcuZLly5d3e5ISiUTSV6nKLAbANsALJk0CwL8qtdvjdiru5eXl7Nixg6VLlwJgY2ODm5sb69evZ/HixQAsXryYdevWAbB+/XoWLVqESqVizJgxlJWVkZeX1+2JSiQSSV+kLltZuTsGeUHjWRe7ulIMhu6N26m4p6en4+3tzZ133snw4cO5++67qa6uJj8/H39/fwD8/PzIz88HICcnh/79+zffHxgYSE5OTqtxV65cSXx8PPHx8RQWFnbvKSQSieQyRZuniLtriBe4uiJUKjwooaqqe+N2Ku46nY6DBw+yfPlyDh06hKOjY7MLpgmVSmVyXvFly5aRmJhIYmIi3t7eps1aIpFI+gj6giJ0aPAY6ApqNfX2bnhQ0u1wyE7FPTAwkMDAQEaPHg3AggULOHjwIL6+vs3ulry8PHx8fAAICAggKyur+f7s7GwCAgK6N8tLSHZ2NnPnziUsLIyQkBAeeeQRGhoaWl2Xm5vLggULOh3v2muvpayLKd+ee+45/vnPf3bpXolE0jtRFRdRjCcubooca509cKfU8uLu5+dH//79OXnyJABbtmwhKiqKOXPmkJCQAEBCQgJz584FYM6cOaxevRohBHv37sXV1bXZfXO5IYRg/vz53HDDDaSmpnLq1Cmqqqp4+umnW1yn0+no169fc36Zjvjxxx9xc3Oz0IwlEsnlhlVZEWUaL5qcH3pXD7Os3I3KLfPvf/+b2267jYaGBgYNGsRHH32EwWDg5ptvZtWqVQQFBfHVV18Bysr0xx9/JDQ0FAcHBz766KPuzfASsnXrVuzs7LjzzjsBpXjHG2+8wcCBAxk4cCCbNm2iqqoKvV5PQkIC119/PUlJSdTU1LBkyRKSkpIIDw8nNzeXt99+m/j4eIKDg0lMTKSqqopZs2Yxfvx4du/eTUBAAOvXr8fe3p4PPviAlStX0tDQQGhoKJ988gkODg6X+N2QSCSWwLayiApbr+bXws0dD0oo7wlxHzZsWJunoLZs2dKqTaVS8fbbb3dvVhdziXL+Hj9+nBEjRrRoc3FxYcCAAc17EUePHsXDw4OMjIzma9555x3c3d1JTk4mKSmpObPkxaSmpvL555/zwQcfcPPNN7N27Vpuv/125s+fzz333APAM888w6pVq3jooYe68bASiaS34lBTRLFDePNrlacH7qRztrx748oTqt1g+vTpbabp3bVrF7fccgsAQ4YMITY2ts37Bw4c2Cz8I0aMaP6ASEpKYsKECcTExPDpp59y/Phxi8xfIpFcelwaiqh3Or9y13j1oFvmknOJcv5GRUW18qNXVFRw9uxZrKysWqT+7Qq2trbNf2s0GmprawGl3uq6desYOnQoH3/8Mdu2beuWHYlE0ktpTBqmczsv7ta+HjhQSkWZge6sv+XKvQOmTZtGTU0Nq1evBkCv1/PEE0+wZMmSDn3g48aNa96DSE5O5tixYybZraysxN/fH61Wy6efftr1B5BIJL0aUVaOFXqE53lxt/F1R4OB+sLuLd2luHeASqXi22+/Zc2aNYSFhTF48GDs7Ox46aWXOrzv/vvvp7CwkKioKJ555hmio6NxdXU12u4LL7zA6NGjGTduHBEREd19DIlE0kupylAOMKl9PJvb1F6Kq1dfVNq9wUUvYMSIEa3akpOTL8FMzINOpxO1tbVCCCHS0tJEcHCwqK+vt5i9y/m9kkiuZLLW7BECxC+PbzjfuH69ECCevyGx0/vb0s4mLg+f+2VGTU0NU6ZMQavVIoTgnXfewcbG5lJPSyKR9DKqz16QNKwJd3cAVI2V2bqKFHcL4Ozs3G4CfYlEImmiKWmYU/AF4t4Ygacu755bplf73MWlLxLV65HvkURiftLTobEsskVpShrmMqi1uFtXdm/l3mvF3c7OjuLiYileHSCEoLi4GDs7u0s9FYmkz5CWBuFhBj77zPK2DAVFNGCNR5Dz+cZGt4xNVR91ywQGBpKdnS3TAXeCnZ0dgYGBl3oaEkmf4bs3z5CjH82vm1fB4jkWtaUqKaIIL/zdLsiqa2dHvcYe29ruuWV6rbhbW1szcODASz0NiURyBWHQC+JWPYA3RTifTQIsK+5WZUWUWXnR76KM6bV2HjjW9VG3jEQikfQ0yS+sZXLdJgA05d0TV2OwrSyi0sarVXudowdO2hK645WW4i6RSCQAlZUEvPoIR9TDyFf7Y11heXF3rC2ixqG1uGsd3XGnlOrqro8txV0ikUiAhhdewbU2jw3Xv0eFrTe2NZYXd+f6IuqdW4u7zqX7ycOkuEskEglQuCmRA4xg4pOjqbbzwL7WwuKu1+OqL0Hn3lrcDe5S3CUSicQs1GcVUG7nx7hxUOfggVODZcVdlJahwYDwaC3uKnd3Ke4SiURiDhyr8hE+PqhU0ODogbPWsuLelDRM49ta3NVeHjhQS1VRXZfHl+IukUiueAw6Ax66AtR+voDi83YzlNCtcJVOqDijiLu1f2txt/JRTqnW5XU91l2Ku0QiueLJPV6KNTrsghRxF+4e2FGPtqLWYjarMxVxtwv0bNVn46ecUq3P6/q3BynuEonkiif7QD4AbuGKuDfld6nKLLaYzYbMPAAcQ/xb9dn5K/Z1hXLlLpFIJF0m/1gBAD5DfIDzBTOqzlrO767LyEaHBo9I31Z99oGKfUORXLlLJBJJlyk/pazcPaMUobX2U1wltTmWE3dDdg55+BMwQNOqz7rR5043crpLcZdIJFc8tRmKuKv9FXG3bXSLNJyznLjbFGRTaBOAtXUbnU0FO8qkW0YikUi6jOFcPnqVptnXbh+g/NbmW07cncpzKHduJ6Oriwt61Fh1IwWCFHeJRHJFo9OBbWk+1Y4+oFYk0bF/933eneFZm029Z0DbnWo1lRr3buV0l+IukUiuaDIywFvko3X3aW5z8bWnDltEsWXE3VBWgbOoxNCv/VoMVTbu2HUjv40Ud4lEckWTmgo+FDT72wFcXFWU4IG6zDLiXnw0BwCbge2s3IEaOw/s66TPXSKRSLrEqVPgS37zASYAa2soU3l0y+fdEYWHFXF3DG9/5V5n74FjN/LbSHGXSCRXNKmnhCLuA1rGm1dYe2BTbRlxr0rJBsAztv2Vu85FyW/T1QwIRol7cHAwMTExDBs2jPj4eABKSkqYPn06YWFhTJ8+ndJS5euDEIKHH36Y0NBQYmNjOXjwYNdmJpFIrmy+/BIyMy1uJielEnvqUPm1FPdqG49u+bw7oj5dWbn7jWhf3A2+fviLXMpKDF2yYfTK/ddff+Xw4cMkJiYCsGLFCqZNm0ZqairTpk1jxYoVAGzcuJHU1FRSU1NZuXIly5cv79LEJBLJFcz+/XDLLfDeexY3VXZSiXHHx6dFe629Bw7drGPaLtnZFKs8cfGxa/+akFAcqCX/UG6XTHTZLbN+/XoWL14MwOLFi1m3bl1z+6JFi1CpVIwZM4aysjLy8vK6akYikVyJvPSS8rvYcrldAOrrQZujpB7At+XKvd6CaX9tC3MotG3f3w5gFxMGQMWB1C7ZMErcVSoV11xzDSNGjGDlypUA5Ofn4++vJLzx8/MjP1/59MvJyaF///7N9wYGBpKTk9NqzJUrVxIfH098fDyFhYVdmrxEIumDHDsGjYtFSrseLWIMp0+DD40r94vEXevsgb2hBuq6nlO9PZwrsql0ad8lA+Aar4h7w/GuibuVMRft2rWLgIAACgoKmD59OhERES36VSoVKpXKJMPLli1j2bJlAM1+fIlEIuGll8DJCfr3t7i45+QokTJAK3HXuzbmdyktBf/WmRu7g2ddDsUDR3Z4jW98f+qwRX3agiv3gADlE8bHx4d58+axb98+fH19m90teXl5+DT6qwICAsjKymq+Nzs7u/l+iUQi6ZBTp5SN1AcegEGDLC7uRUUXiLu3d4s+4d795F1tUVlUj48ogE500cZOTYYmBPuctC7Z6VTcq6urqaysbP77559/ZsiQIcyZM4eEhAQAEhISmDt3LgBz5sxh9erVCCHYu3cvrq6uze4biUQi6ZAPP1SCzB97DOHubnFxLy5WxN3g4QlWLR0ZKs/G5GF55vX75x1UFsXWAzv2uQPkOYXhXmwht0x+fj7z5s0DQKfT8Yc//IGZM2cycuRIbr75ZlatWkVQUBBfffUVANdeey0//vgjoaGhODg48NFHH3VpYhKJ5AokNRXCwijS+PLDd+7cqivF1oLmiothCAWtwiABrHwVca/JLsHGjDZLjiox7s4RnXs0Sr3CGHvmJzAYmvPeGEun4j5o0CCOHDnSqt3T05MtW7a0alepVLz99tsmTUIikUgAJa49KIjHHoPQCndsKQO9HjStc56bg+Ji6KfJR+XbWtxtGsW9O6Xu2qLqpBJg4jm085V7bUAYdqfrIDsbBgwwyY48oSqRSHoPmZlkqoL43/+ghEafd3m5xcwVFYG/Or/VZiqAXT/L5HRvOKOs3L2Gdr5yFyGhANQnme6akeIukUh6B1VVUFLC57sGEBUFA4crBSss6XcvLgZvfdvi7ujnjA4NugLzirsqN4dqlSMaD9dOr7WJ7nqsuxR3iUTSO2hMNXC0PIhVq8AlWBF3UWI5ca8qrMXRUNnqdCqAm7uSGdLcaX9ti7IptgsAI8LH3YcEUIudXLlLJJLLmEZxVw8MYswYcA1SxL0s3XLiripoO8YdwNVVcQ2pSs0r7i4VOVS6du5vBwgcoCaNUEiT4i6RSC5XGsXdJiwIAI8QRdyL0ywn7tYlnYu7utx84l5f2UD/hjS0vsaJe0AApBKGfbYUd4lEYk4MXctI2BVERiYNWOMVo5yL8QlXxL0iwzL5XRoawLu2MetkG5Eozs6KuFtXdt1+QgKcO3f+dcX/+ye+FFByza1G3e/iApk2YbgWnVGihkxAirtEImlJdbWSjfH665U0AM891yNma05kkkV/QgcrstQvWhH36hzLrNyLi2Eg6cqLgQNb9Ws0UGnlgV0Xc7oXFMCSJfDuu40NZ87g8c4LfMVNON440+hxSjzCsDI0wAUn/41BirtEImnBieX/guXL0R1LBjs7OHCgR+xqT2eSSRBhSoAIbv721GJHwznLiHtRkSLu9c6eyjK9DartPLHvYtrf3MZMvcnJgBDwwAPoVVY8xhuEhho/TnW/xjck1TTXjBR3iUTSgqrfk8hkAFf5nEY/+irlAE0PYJ2jiHuT8KlUUGnljr7Yciv3QZyhvt+gdq+pdvDGXlsJNTUmj98k7idOAD/8AJs2sT7+79S4BeDhYfw4hhAp7hKJxAy4nUsh1SqS/Ykqtp0ORLSRstvsNDRgX55HjiaoRT6tWjt31OWWdcsYglq7ZJood230xZ89a/L4TWUsTp0C/cafwNmZ/9o/QGioUVGQzTiG+tOANYZM6ZaRSCRdxWAgoDIF7aAIXngBfk0NRFVYaJGc5i3IykKNoN4vqEUKFa2TO7ZVltlQLSnUE0Qm6tD2xb3KU4nc6Uq5vyZx12qh7tAJiIri1BmrZreTsQT2V3EOP+rSTSt6JMVdIpE003AmGwdRgzY0gqefBr1v4zI6t2ul3oymUTxVA4Natru546QrparK/Cbrz+Rggxa7yPbdMnW+3Rd3AE1KMvrwSDIyMMnfDko45Dn80Gad6/ziC5DiLpFImsnfngKA7bBIVKoL0tJa2DVjSFfE0yGipbhrvNxxp7QrXpFOUWUokTI24e2v3A1+/dCh6ZK45+ZCcDC4UoZd2TlKfCIxGLom7nn4t4ypNAIp7hKJpJmK308A4DVOqbbmEKas3EWWZTdVK45lYkCFd1z/Fu22/h64U9oVbe0U29z2wyCbcHa3IptAREbXVu6hoTDVT3lPMx0iAdPFPTBQWblbF0u3jEQi6SK64ymU4kbwKCXXinuMsnKvPGnZlXv1iUzy8CcksmXmdMf+7rhSQVaGaQd4jMGp4Ax61B2m0g0OhkyCqE/tmrj36wcTvBRxPy6iANPF3dsbiq38sKssBJ2uub2zbRAp7hKJpBn7jBRSNZF4einhHAOGuFCJE1UnLLtyFxktY9ybcO6vHGTKP1lmdpuupekU2QWCTfulOCIjFXE3deUuhCLu/v4w3DaZOmzZXxiMs3Oran6dolaD8PdHjVBORjUyfHgn95lmRiKR9GW8ik6Q7x7R/HrQIMgmEG2GZVfudvmZZGuCWtWhVns2Jg87Y/6IGZ+qdIqd23fJAERFKeJuW5SjhL0YSXGxcrm/P4Q0nOAk4fy8RUNYmGlhkE3YB/spfzT63WtrlRDLjpDiLpFIFEpL8WjIpyrwvLgHB0MOAajzLLhyNxhwrcii0iOodSU5d0XcK8+aP9Y9oP4M5Z7tR8oAeHlBiVMQamEwaVO5KVLG3x+8i09wgkhSU013yTThHqV86tWeUQZOTu487Y8Ud4lEAkD90ZMAiIjI5jZbWyh1CMShxIIr93PnsBZa9AFBrfsaxb0217zirq+qxU/kUePb8codQBVsejhkk7gHetRgm5dBMl3ztzfhN0xZuecfUVbuR492fo8Ud4mkF6LXNx5b70GKdikGneIjWrTXewXgWpNrclZCY9GdVWLobQe1UXau8Zy+vqj0wr3EblNxTBFqXf/Oxd0pWhF3U/zuTeLev+YkKiHId+9apEwTA8cq4l6aooj7kSOwVnNTh/dIcZdIeiFr1kB0NKSl9ZzNmgMp1GOD39iLBC8wECv0LTbzzEnxcUWw3CL8Wnc2rtxdRWmLQ0HdpfrYGeWPQR27ZQB84pVomupk48W96cyXT7HygWkI7564h0TZUoI7dY1umaQjeq41/NDhPVLcJZJeyMmTSsTFzp09Z1N1MoVUwgiLtGrR3rSirk21jN+9MlURd9fw9sXdnVLy881ns+Fk5weYmggfasc5fKlMal/c9Xp4/31loxOUlbuLC9ieOQFqNc5xShiQqakHmrC2hhIbP8S5cwgBpYczsRMdx0JKcZdIeiFNqbv37Ok5m045KZyxiWjS02Zco5VY94JDlvG712cq4u4e0boaEra26G3t8aDErOJuOJNOLXY4hbbxgXIRTeGQujPti/uvv8J998Gnnyqvm2LcSU6G0FDuuNuWJ59ss+CT0dS4+mNXkkdeHviVde6zk+IukfRCelzctVq8K05T7B3Rqst7mLJyLz9umZW7PuccJbjjF2TbZr9wczf7yt367BnSGYiXd+dxiQEBkGMVhG1e++J+5Ijy+9dfld9NMe6cOAGRkQwfDq++2rUwyCYMPn641p0jMREikeIukVyWNKVQP34cyst7wGBeHhqhx9A/uFVXULw3DVhTd9oyK3d1wTny8G/3cI/aw/zi7pSdwmlC8PTs/FqVCmq8gnCrPNtu/OHRo6DCwK+/Ku603FwI9NUqOdgjI9u8x1RsBvjjTx5rvxZEcgKDT8dfA6S4SyS9kKwsRROEgH37LG+vPl3ZAbQP6deqz8NLzTlVP4sV7bAtPUepjV/rGPdG1F4eeGnMKO7Z2XgWnGCnelJ7BZhaIQYEYWOoh8LCNvt1e/ZTjisD8vZy8qSyco9XH1TSBcTFmWXaLoP9cKCWLesqGWaTjDqq4w8NKe4SSS+jvBysK4t5dcC/sUbbI66Z0uOKuDsNbi3uACUOgdgUWWbl7lSZR6VjB75vd3fzivtPPwGwz32m0W4Sh0glHLKtTVVtg+CBtEdxpoobWMe6dUrel2EVO5QLJkwwx6zxilHeI8fKPMINJzr9RiDFXSLpZWRlwe38j+t/epjP3O5nz25hcZuVJxVxd41sW9yrPQJxrbTAyl0I3OrPUefWsbh7mNMts3EjxXYBFPlGG32LZ5wi7rl7W4t73r/WcJXYjc7WgVnWW5o3VUNydsDgweDX+aatMdgFK6dUh3MIJ125khuhA6S4SyS9jKwsiOEYAAvKPmTE9tc7PWreXRoyctFihW+0V5v9Br8AfLQ56HVm/qCpqsLeUIPOu2NxdzWYKVpGq4XNm9njNqs5OZoxBE1UxL38yEXiXleHx6tPcZihlCx5ghjdQbKTSlGjx/fUTpg40QyTbqTxQ2IKjbu25lq56/V6hg8fzvXXXw9Aeno6o0ePJjQ0lIULF9LQ0ABAfX09CxcuJDQ0lNGjR5ORkdGFp5BIrlyys2EISdSNmUT6iAU8X/ckOe93fGCluxhycsnDn4D+bUuCVXAgjtSQm1xmVrv6HCUMUu3fsbjb66ooPmd84q4W3HQT/Pe/yt+//w4VFayvn9miVmtnDIhxpRQ3dKnpLTveegunwgz+ZPUaHgunoxYGJrGdISRhVV1uXnFvzKo2la3Ka3OJ+1tvvUXkBYM99dRTPPbYY6SlpeHu7s6qVasAWLVqFe7u7qSlpfHYY4/x1FNPmfoIEskVTfZZA9Ecx2ZELPUrE0gjFM2/XreoTeuCXPLV/drdYHQMV5Qw/4B5XTNNx+mt+3cg7o0pCCgpNiUxo0JtLXz9Ndx/P6SkwMaNCI2GNaXTmDLF+GE0GsiyC8MxN7VlxyefcMR9MvlDpmE1bjQGewemspUJNJ4+M6e4u7tjsLYhjDSEqyutUmhehFHinp2dzYYNG7j77rsBEEKwdetWFixYAMDixYtZt24dAOvXr2fx4sUALFiwgC1btiCE5X2GEklfoeZEJs5UoR4aw+BhDhyyHo1tlmXzENiX51Hu0L5YuMUoR/DLk7LMarfipCLujiEdiHtjpaRBnGkvWKV9mvIA1NfD4sWwYQO5QWMpx43p000bqsh9MD5lJ8836HRw6hS7GkYRGwvY2KCeOIGZ1luYqtmBGDAAgtpIhtZVVKrmbziqyMhOg+aNEvdHH32UV199FXVjrFJxcTFubm5YWSnHlAMDA8lpTIeZk5ND//5KqSwrKytcXV0pLi5uNebKlSuJj48nPj6eQpP/i0kkfRfb1CTljyFDUKuhPjAE9+psRaAshFtNLjVubW+mwvn8Kl2pSNQRtekdpB5oIjxc+cVJ0/3uTWl6775biSk9coStNrMICVHSGZtCVUA4/g1nz+cYSE8HrZb91ZEMHdp40dSpDNYmcw0/oTLnqr2Jps1ZI2LnOxX3H374AR8fH0aMGNHteV3IsmXLSExMJDExEW9TS5NIJH0YjxxlM5VoJZrDKiIUNaK1v9dc1NXhqitB692+uNsF+9GANeauVK3NOocODV7hHZwmCg7GYGXNYE51Xdwff1zxvQMrz840edUOYAgdDEDN0UbXTIpSTDyFCGXlDjBtGgBO+grzumSaaBL3TiJlwAhx/+233/juu+8IDg7mlltuYevWrTzyyCOUlZWha8zBmZ2dTUDj7kRAQABZjWendTod5eXleBpzDEwikSAEBJYlUeISpGSeAtziQwDI23XaIjb12UqmQVVA++KOWk2BbX/s881cqfrcOfLxxa9fB1JkZYUuKKR7K/eAAFi1ihMvfcuumjiuvtr0qdrGKt8ginc3lkBqzMncQtyHDWtOdmYRcW/ys5tj5f7yyy+TnZ1NRkYGX3zxBVOnTuXTTz9lypQpfP311wAkJCQwd+5cAObMmUNCQgIAX3/9NVOnTkXVnYQKEskVRGkpROqPURY4pLltwGRF3It+t4y4Nx1gsgnuQNyBMpcBuJSbd+VuVXyOQo0fdnYdX6cOH9z1lbuTk/JB6ezMl/U3oFLB1Kmmz9V9lJLSsfZwo989JYUyez/s/dzw8Wm8SKOBq69WsoYNHmy6kc4wp1umPV555RVef/11QkNDKS4uZunSpQAsXbqU4uJiQkNDef3111mxYkVXTUgkVxzZZxqIIIX6wTHNbWFXeVOJE7XHLSPuZckdn05totY7CN+6TMwZH2Fffo5y+84P+VhFhxNKGgV5JhYMyc7mwpjHX36B+HhaZb40hgGRjmQRiDh5fuWeqrlg1d7E22/Dtm3dyxLWHjffDP/3f0ZtGFh1esUFTJ48mcmTJwMwaNAg9rWR9MLOzo41a9aYMqxEImmk9PdTWKPDatj5lbutnYp0uxCsMy0j7jVpiri7R3cs7ob+A/BPzqUoT4t3P2uz2HapPke11/DOLxw8GDvq0aefBTrPwd5MTg4ZugD+93eYMQP27lW0sSv4+cF21WBCzirJ9kVKCodqbjm/mdqEtzftZkHrLlFR8MorRl0qT6hKJL2IhoNKpIzLVTEt2iu8QnAvsYy4a8/mUo8N/tEeHV5nExqEBgN5B3JNtnHs2PmoxGYMBty1+TR4GHE8vzFixi7zZCcXtkRk57DjdAB//SuMGqUU1ejKZiqAWg25zuF4FJ2E/HxUZWUkGSJbr9x7CVLcJZJehFVKkhI9Mi68RbthUAiBunTKS82fh0CVl0cu/TrNbe4crYRDlh42fVN17ly4666WbaKoGCv0CF8jxL3Rf+2Sf8p4owYD5OWSQwCvvAL/+Q888QSMH2/CxC+i3HcwTtoy2LULuGgztZchxV0i6UW4nj3GGatwNA4tC1c4xoZgRz0nt5o/M6NNcS4ltv7tptxtwitOEfeaFNM2VevrISND8XdfeKSlqbyeVaAR4u7jQ42NK76lJqzcCwtR6XTkEMDYsfDAA/DPfyol67pKw8DGD9316wE4bRVBROv6Jr0CKe4SSS/CtyiJLNchrdr9rlIiZnJ3mt8141SRS4VTx/52ANfGU6qGDNNW7pmZSoinXg/ffHO+vawx9YBdsBHirlJR4jWY/nWnaIzA7pymg5UEmHxgqT2sopRvEGLDBmo1jjhHBmJjY56xzY0Ud4mkt6DV4leXQZlveKsur9GKuFccNr+4e9TlUufeubirHOwp1nhjnWvayj298eyVtTV8+eX59qo0Rdydw4xLiVsdEE44JykqMtJwo7if0wQq9UzNgNuwYBqwRlVaSqo6nNhhvVdCe+/MJJIrDMPZbDQY0LdR6k41oD86lRUizbziLqqqcTGUY/AzTv2KHQfgVGLayr1J3O+4A7Zvh3OKplOfoRyeco80Ttx1gwYzgCwKMmqMM3zBASaNxpQZt8+AgRrSCAXgmLb3+ttBirtE0mto2qi0HdxGsikrK0pcgnHKP23W3O6VpxSBVfc3TtyrPIPwrDZ95W5jA48+quxxrl2rtOtzz1GFI74hTkaNo45UvtFUHUrt5MpGcnLQocE5tONao6YQHAwnUeZxgt4bKQNS3CWS9jlzBkaOVHYDe4CSQ4q4u8a2nUmwPjCEIN1ps06n6Ghj7dRBxom7zn8AgfpMqquMP8mUng6D+9cSE6ElOvq8a0ZTcI58lR+ursaN4zBcEVVtspERMzk5FKj9GDDQTMt2lIOnaSrF755CROsY916EFHeJpD127IDERHjnnR4xV5uiiLtvfP82+63DQwgljaRj5jsi2lRezyXCOHFXDwrCkRpyjpUYbSM9HdYWToA772ThQti5E265UYt/2g7O2oUbfZDTfaTiDtGkdhAxs3WrUsAU0GflkGUw32YqgJUV5HoqZxDyPIbga74vBWZHirtE0g4itTGH+kcfWTTdbhOGjExy8WdAmG2b/S7DQ3CjnOyjxgtrZ9SeUdwyXrHGibtjhBIxU3TQeNdMzpl6QisPwWefce/EE1x3HfTf/SV+uhz2xj9o9DjOfo5k0R/7rHbE/dgxJSvjq68CoMvIIYeApnTwZuNo1C2MYxeOI3ppDGQjUtwlknYo/P00etRQVIRYt97i9mzzMsm1CsLRse1+h5jGiJlDXdxUfe45eOwxLkwOI7JzqMEev3DjfCPuQxVxrzpu3KZqZSW4l6ShFgYQAp9VL/PD94J/+P4TERXFn7bNNHr6KhVk2YbiUtDO83//vfL744/BYECdl23WMMgm+g+0YjfjerW/HaS4SyTtIlLT+JUpZBDEySdWWrxItUtpJqUuHVTuCVHE3ZDaRXH/7DN480146SUAyrceYMjvH5JsPRQbW+N8I14jlPk1nDZu5Z6erhTZAGDyZGUOK1fCkSOonngCldq05FolHiF4lrfz/Bs2KPGW6enw009YV5dbRNybiitJcZdILlOc809zWjOYlAn3EJGzhRfvtGCpO4MB79qz1HoPaP+aQYMAsMnqgrgbDEqhDUdHeOYZzj30Iobp11Csd+fsP74yehgrPy/qVHZostpeudfUwHvvKckYQdmTjkApasH77ytO6/vvB19fuO02kx9DFxSKh7YAUVHZsqOoCPbsUUJynJ3hxRcByNcEdFZq1GRCFdc/cXHmHdfcSHGXSNqipASH+lLKvUKY+cWd6FUa3Nd+aDFzhrx8bGjA0L+DlbuDA5VO/niWnzZ9CyA/H+rrecfveRLtxuH3n2eoxZ7StVuZ/0jbG7htolJRYDcAu4KWK3chlNOnkZGwfLniAYLzK3eDX2N+86VLlQ+ahx4C27b3FjrCNkr59lK496IPuE2blEncfLPy89tvAOj9AjpNq2AqCxfCTz/BkNYHiXsVUtwlkrY4rYiHtn8I9OtHxuAZTK/+lvJyy5grOqCshG3COi6oXNsvhBBOc+aMiQYylfH3loazcua37B7xEFbbtzJ8vum7jRWeA/EoazmB11+HG28EV1cYN05JvaLXK+IepT6JKqpx8/Gvf1VW7g8av5F6IZ6jFHE/t+v8tyitFiq/2IDOy5eTjnGIxUua+6yCAi4eotvY2MA115h9WLMjxV0iaYOmk6BWEcp38Ib4sYRzilMHKju6rcs0HWByielY3FWhirinmeghqj+ZAcCYhUGs/NabqxL/hc/4rlUK0g6KIFSfQsG585sQP/6o+KAPHlQ8I0VFSuLE9DOCcFJQNabsxc9PKWZhbHD7RQRNVcS98oI0DHcv0aHbsInVRdcSEaXmrcRxzb4Tp3Dzi/vlghR3iaQNqo4o4uE6XPFzu0xUCkoU/nLEIvaqkxVx9xnZsbg7xoYQQC7pybUmjV+UmAFAwFUdj28MtsMicaSG09uUWslCwNGjynkvq+NHmPvlrUy12cU330BFWgEuhnLMlTrRJ9SFQpU3hsYPXyGg7MfduFPG4MeuZ8IE+PuLKsrufYptTMI/zLjTr30RKe4SSRvUHEsjh34ERToA4DtL2T3T7jtoEXuG9ExKcaN/tEuH1zWFQ5YfMs0vU3U8k2I8CI937vIcm/CeqNTvLNqRDCju/Iaich469SDExWH99Re86PEa334L9pmNm6nhrZOhdQWVCs45heKYq3x1SUuDq8o2oNdYM/5v03n9dSguhmX77mYK28weKXM5IcVdImmLtNOcJqQpQAWr/v4UWfnidOqQRcxZ52WSYxWEvX0nFzaGQ2pTTIuYERkZZKqCmyM9uoPXxCgAGo6cAJRV+0v8hdjf3lV2U2+/nRElP1OYVcuAusYwSDOJO0C1bwjelcrz79oF09lM3Yhx4OxMfLzi+2+q9CnFXSKRtMA+N43ThLQQhyzP4QTkW2bl7lKSSYmzES6TRnG3PmuauDsUKjH0ViZVTW4blbcXpVZe2KafF/dJbEc7daZS7uiOO7BuqOEa9RbCOYnexg4GdBDiaSohIQQYsijMrufgllKGcRiH66Y2d7/wAs0RMuY+nXo5IcVdIrmYmhpcqvIocg1tEa1XERpHSEMy9eV15rUnBF41mdR4GSHunp7U2brgUWpCOKQQeFdnUO8X3J1ZtqDQKxLvQkXcUxPLiSIZm4ljlM7Jk8HZmXv91hNBCg3BgzFnPKLj0FDUCNK3pqP/dQdqBKopk5v7IyPhzjvB05NenfvF0khxl0gupjHOsC4gpEWzesRwrNGRtSnJrOb0xWU4i0r0gUaIu0pFjV8IgzjdnCe9MyrTi7AXtViHdn8ztYm6gVGE6ZIpLBCwfz9qBIwerXTa2MCsWUyp/p4hmhNYx5g3B4vPWOW/S9pPpwnL3YbWyk6pfn0B776rfKMwNilZX0SKu0RyMY1xhuqwluLucbWyqVq+1byumcJEJVLGWPEVg5RwyFQj05qn/5oBgOvQ4C7Mrm1shkbiQSknthfgl7FXabxQYOfOxb48nyB9OlZR5vO3w3lxP/VjGpPZRs3Qq1odiLK2xmzVly5XpLhLJBdRl6z4sx1iW+4+Bk8ZSBmuiEPm3VQtPmhcjHsTDjEhBJPB6VN6o64vaPzw6DfWfCt3z/FKxMzBT08w0rCX8n6R4OZ2/oJrr6XZwW/GzVQAlY831WpnQsv2M5QjOF432azj9xWkuEskF1F1+DQluBMwxL1Fu6OTihO2w3E70/WVu8EA27bB3XcrK8s5c+D4j4r4escbJ7520SHYoKXoUJZR11cnZQDQ76rgLsy4bbwmKOKetyWZMexFP2pMywvc3GDSJOVvM8W4N6NSUeQWwo2sRY3A6urJ5h2/jyDFXSK5CP2pNNIIbQpMaUGe33ACS46CTtelsbeP/RP+U8Kp/nQdY0YLvHZ8w/TfniWLQAKGeRs1hipUmVjDiZYRM1u3wvDhSoRIjEcOLz+rbPyKjEyqNC6oPdy6NOc259A/kGq1E5OqfsCLYlymj2l90R13gIeH+cUdqAsIxZ46GqzsW/nbJQpS3CWSi7DNUsIg2xL32sg47EQdhuSULo3d7+hGwjnF53Xz+CYtlv+W34gICSPxn9uxszdy969xYlaZLcV97VpISYFbYpPZVz6YkOcX8c034FCYQYlLcJfm2y4qFfkekczgJ2Uu49sQ90WLoKCAdhPUdwPryMbc9kNa+9slClLcJZILqa/HpSSDLLvBLVzITdiMVtIQFP9iut9dV6cjuC6FHSMeg7feUhKwPPEEHsm7mPfEIOMHCgxEp7bGrbhlOOTp0zAiopqX027C3lDDzazh5duP41efSUO/YJPn2xm1wZFoMFBn5QjR0a0vUKlAY776pRcycJoi7p7zJ1tk/L6AFHeJ5ELS0lAjqPBvexPQb1I4ddhS+ZvpOWayt6VhSwOq4cPg4YchLw/++U8ldNAUNBqqfQe1CodMSxW8WHo/nDgBX36JwcGR/9O/TDAZWIeYbzO1CasY5aRqftAoi4l4e6hGxoOVFarrru1Ru5cTnYp7XV0do0aNYujQoURHR/Pss88CkJ6ezujRowkNDWXhwoU0NDQAUF9fz8KFCwkNDWX06NFk9FDleInELJw6BYAIaztjYvRQK44TjerYUZOHLvj1OADu49tY5ZqIIVgplt2UHVKrhREZa5mUuRqefRZuvhn1/ctZoP0MFyrxHhncbZsXE3C1sqnqOLUNl4ylGT4cysp6f8WMS0in4m5ra8vWrVs5cuQIhw8fZtOmTezdu5ennnqKxx57jLS0NNzd3Vm1ahUAq1atwt3dnbS0NB577DGeeuopiz+EpI+zZw/odGi1SqUfS1J5QMmF4jexbXH38IB0p1jcz5ou7nUHjmNARdDMyG7NEcA2LooIUjidogWUIkuzDD9Q6+IDzzyjXPT446gavxU4RJp/5e40dRT4+OC1+Dqzj20UFvDl9yU6FXeVSoWTk5I2U6vVotVqUalUbN26lQULFgCwePFi1q1bB8D69etZvHgxAAsWLGDLli2ICwrySiQmcfgwXHUV/PGPPPmk5QMjSveeIg8/hk1sPztjdUgsbvX5SjpEE7BNTeKs1SCcfR26O03sxw7Hjnqq9ispAE6fhjgOUhs54ryLxN9fibkEy2TQ8vNT3oNx48w/tqTbGOVz1+v1DBs2DB8fH6ZPn05ISAhubm5YNR5SCAwMJCcnB4CcnBz691fKdllZWeHq6kpxcXGrMVeuXEl8fDzx8fEUFhaa63kkfY0DB5Tfb71FZcI3HD8OlZaplwGAOHmSU4QzYkT719iOGgpAxW/HTBrbq+A4eR7dd8kAqEYo7gib48rGbvqJOqJIxnrU8JYXPv+8UiZp+PCLh5D0cYwSd41Gw+HDh8nOzmbfvn2kpHQtDOxCli1bRmJiIomJiXh7GxffK7kCOXYMHByoHjKK18ruYiBnmtziFsE1/xRFnoNx6GBxHTAzBoC8TcZvquprGxhQd4rqYPOIO2Fh1Gkc8MpSDlTV7juGFXocJ1zkg/bwgMceM2viLsnlgUn/xd3c3JgyZQp79uyhrKwMXeNBjuzsbAIClHJWAQEBZGUpJ+d0Oh3l5eV4enqaedqSK4ajR2HIEP57zZcIVHzMEsywtmgTQ1EJbtoiRFjHx+VjpnqTiz/1icb73XO3ncIaHZqhZqqqrNFQ4D+MkIpDNDSAzXFF5NXxcoNRotCpuBcWFlJWVgZAbW0tmzdvJjIykilTpvD1118DkJCQwNy5cwGYM2cOCQkJAHz99ddMnToV1ZWcmk3SdZrqt8XG8tnuYL7wfZTx7OLMEcv4ZTJ/Ub4SuI3quLaomxukOcTidKZjcZ8/X6knClCwTYmU8TBDpEwTNeHDGc4h0k8b8Dp7iCprtyu7OoWkBZ2Ke15eHlOmTCE2NpaRI0cyffp0rr/+el555RVef/11QkNDKS4uZunSpQAsXbqU4uJiQkNDef3111mxYoXFH0LSRzl3DoqLqRoUy++/g8vUeNQIGvYdtoi57F+USJngGZ0nuioLGkpAebISg9gGZ8/Ct98qtaDPnoX6xCR0aIwa21isRsfhTBU5O04zqOwgeX7Dr+wct5IWdFqXJTY2lkNtZMEbNGgQ+/bta9VuZ2fHmqYaVxJJdziqrIz3VMUgBETdMQI+B6eTB4AJXR724EHlpymQpImqg6fQYsWgaZ2X77EaHovtiQaK95zCc2Lr1fjatTCdnykzePLGGyO4Le04GVahhPradXneF+M5bTi8BOe+28dV4igpgx8w29iSyx+5yyLpvTSK+1cnYvDxgaEz/Ch36kdgwQH0xmW7bZN//xuWLVMOiF6IdfpJzjkMQm1r3ekYPlfHApD1QxubqkJg/+bL/MwMdqomcvy9nXjkmS9Spgm3cdE0YE3Azs+xo1762yUtkOIuMYkNG5QSZj1ydOHYMUS/fnz9qyezZikBH+WhIxhuOEB3Dj7n5Cjzb9wyApTDUT5lp6ju17G/vYnwOeE0YE31nov87no9VUse5L6zfyF5yM2I/v1ZW3ctQdo0qoPNtJnaiMrWhjP2QxhfuREAt8ky3FFyHinuEqMpL4elS+Hjj6FHjiYcPUp5UCxlZTBrltKkGjGCCFJIPVTV5WFzc5XfX311vu3QAQNhpGI1xDifuLOnDadto7A7dZG4f/opTqvf4VWexGrN59jt2kKlgy8aDFgNNe/KHSDXPw4NBmqwp98U8xbFkFzeSHGXtEtFRctDmM8+e/51crKFjWu1kJxMupPi/mgqz+k2bQRqBGXbDnd56JwcJVfXrl2Qna20/fLfs9hTh88441buAMX9YulXdJG4HzhAjcaJ/w15hcERaggIIPeTrazULKff4uldnnN7VA9WVusnbYdiZduzybskvRsp7pJ2eeghCAqC116DI0fgP/+B96P/xVamcCLZwn6ZU6dAq+VAQwweHso8AJwnNx4dbTq5aiI1NUq+qT/8QXm9Zo3ybImrlU8rl5HGr35FbCz+hhzyks6fwK4/lEySPpKbbj4ftRI/fwBL698hapx7W8N0C81Ixc+e7SP97ZKWSHGXtMuBA0pk3R//CGPGwG3237Ds+CNMYRsFv6d3PkB3aNxM3Zwfy4gRF0T4+ftTZOOP2+muiXuTS2ZOxCn+GfAGVz13DQEj+/G9oTH5lQlVg7ynKt8qUteeX73rjh7nONHceGPLay2VEddjyjAOMYzMoXMsY0By2SLFXdImej2kpiqr948/homOB1jVcDsMGKBcYOYi0a04ehRhZcUPaRGtsrrm+o0guKTr4v5/vMK8P4XzRM7jOFbk8p12Jok3vgS//AK+vkaPNfAGJcdM6Y5GcS8txbE8jxzXKCK7n/jRKAYPtWeMzSGsrpvRMwYllw1S3CVtkpEBDQ3KQnbxgmp+sp2DlZ837NiBXqXBPb3rRaKN4uhRaoMiqNHZtEriVR0xgjB9CkWZ1SYPm5MDM9lEXWg0WTvSiSGJVVf9l7iv/gzTppk0lu0AX0qsfLA6roi7SFYyNNoNj+qxs0QeHnD8uLLRLZFciBR3SZs05W8JDwc2blSWvB9+CEFBFPtGE1p1iPJyCxkXAvbtI8tHUfWLxd1m7Ag0GMjZcNjkoXOzDcRxENXEifSfEMxXX8EXX3Q9r1ahfyx+hUfR6SDvFyXFwIBZ5o+K6YjQULDuPDRfcoUhxV3SJieVk/iKC/qbb8DLC6ZMAaAucjhxHLRYAi/S0qCoiH1W43Bzg4EXHRj1nqmoffWORJOHrk8+jSsV2IxVxrjpJmjMUN0lDENiiTIkcfyIjsLtyVTjQPz8AV0fUCIxE1LcJW2SkgKenuDpVA8//ABz50Jj/n67q+LwI5+MPXmdjNJFdu8GYEPJWOLiWqdLCRzVj2xVIFaJv5s8tONJxVeviu8gYbsJeEwdij11pGxIQyQnc9o6koEh8p+V5NIj/y+UtMnJk42r9l9+UapjXBD+4XG1ssNZs8tCfvfduxEuLqw7FdVm0Qy1Gs74XUVA5m6Th/Y5e4AGlS1Em8d14jNNiZgp3noUn8LjVPbvOX+7RNIRUtwlbZKS0uhvX7sWXFxabDZajRiKARXWSRaKmNmzh8rosdRr1e1WRNLGX0WALpOiIzkmDR1cfIAs91izOalVUZHoVRo0u3fSz5CDbVzP+tslkvaQ4i5pRWkpFBRAVJgW1q+H2bOVI51NODtzzikM72wLrNzLyyEpiVSfq4DWm6lN+MwdC0DaJ3uMHloYBJF1Bynobx6XDAC2thR7RzBXq2RCDZwRZb6xJZJuIMVd0oqmzdSx2h1QUkKrEzlAcdBwwqsPUldnZuO//w5C8Jt+LC4uMGhQ25eFLxxGLXbUbjHeNVN+8DRulFMVbkZxB/TRQ/FDycvgN0WKu6R3IMX9cqSgQDlZZKHUjE3iHnX8K3BwgBmtD8joY+MIJpPT+0vMa3zPHlCp+Oz0aOLj2w9RtHGyIdVtJJ6nOhH3fftg504AKrYpm6kizrzi7jZR8bs3aOxkJSRJr0GK++XI8uVK3l0LVYpOSYEAzTlc16+GW26hrWrRzpOUTdWCn8zsd9+9m7rBMfx+woU5nZyorxxyFRE1BynNrW37gqoquP56mD4dDh1C//sB6rHBcZR5/eL2oxVxN4RHWi7PgERiIlLcLzd27FDizgH27+/eWHo9JCUprpcLOHkS/ub6OqqGBvjTn9q8td91SjbChr1m9Lvr9bB3L8ddFH/7ggUdX+46cyw2aEn+pJ1UBG+9peQmdnKCm2/G6cB2jhJLv2Cbtq/vKrGKuNsNly4ZSe9BivvlhMEAjz8OgYHKajrR9EM8Lfj8c4iJUQLavbyUas46HXlJxfyh/B1l1R4W1uat9oGeZNsOwvag6eGIAIWf/0JaxHW893IpH38MRUUo5+grKvjm3FWMGwcBAR2PEXK7sqlatrGNTdXSUvjHP2DOHKWYaXo63un7OMAI/P27NOX26dcP5s1rc29CIrlUSHG/TBACXo7+Hxw4wP+GrKB04PDur9z371c+JF57Da6+Gt56C8PcG7jp9MvY66vhz3/u8Pai2KkMLd3G2XTTa96lvLKO0JM/EvaXBdxzp5Z50yow3H0PBmsbVmdN5qabOh/DPsiHLLtQHI+28QHzj39ARQXi+RfQXzUBXnwRgGTHkdiZr4ypgkqlfJuaN8/MA0sk3UD0AkaMGHGpp9DrOXuiSmTTTxy2GSnsbfXidR4VOlt7IbTarg86bZoQI0eef/3uu8KgVgsBIiNuXqe3577+mRAgPn1sv8mmd7tcI6o0zkKASJuwROxgvNCprcQXf1gvQIisLOPGORCzSJzDR1RVGs43njsnhIODqJt/qxg5UohJk4TQa/Xi76PWi/joGpPnKpH0VjrSTrlyv0yo/fs/CSAX3niD4lI12b7xaOpraTjcjZJIx4+3PKl5332sW7yOo8TQ8PTfOr3d/w9TASj/ZotJZqurwbcilbTw6+AvfyFk58eMU+3mD3zG49vmMG6c4nkyBs2EcfhSwKnvT55vXLsWamr4w/GnSUyE7dvh49Vq1hnm4Blob9JcJZLLFSnulwM5OQR/9SpfcRMDbh2HvT3c8OJIAH5+qYt+9+JiOHcOhpwv2qzTwaNbZvPQxKOEzY/pfAxfXwp8hhCSuYUcEw6K7v+tgSAycYgNgxdegGefpXb11+zudxO5uRjlkmnC55bGD5hvtza3aX/aSq71ADakR/Hjj3DVVcq+8JkznfvxJZK+ghT3y4Gnn0al1/GG7yu4N1Zqm3BnKNVWLuR9t5+MjC6MeVxJT3vhyn3dOjh7Fh57zPhhrK6Zynh2sf6reqPvSd6QjgYD/SaFKYHszz2H4+3zWL1aCTxZuNB4+37jQshSD8BpX+O3B4MB/ZZf+Uk7la/Xqpg5E/79b2XDtqRE2fuUSK4EpLj3dg4cgIQEPvV+FNdhF+S+VavRjIonTiTywgtdGLcNcX/zTeVE6OzZxg/jcdM0HKjl1Oq9Rt9zbmcqAI7DWkbiTJmi1DP18zPevkqt4oTfVMKyf1WiiQ4fxq66hESXaVzXWDkvLg6WLVP+lit3yZWCFPfezvvvI1xceKrsLxd6UACwGx9PrDjC3u3Gr5qbSUpCuLiw+2wgxcVK4Mxvv8HDD5t4DmfSJAwqNZ6Ht1BQ0Pnlej1okxVxJzTU9Hm3QVn8NFz1pdT/fhixRXHP6CZObZGd8cUXlUhFE4stSSSXLVLcLzF1dfDBB4rotcmBA9QOGUVBvWvrLLUjR2IttNifPnbxOaROMRw7zgl1NOPGq/DyUiIhnZ2Vg68m4epKTfRIprCVzZs7v/z4cehfn0q9g5sSX28GHK9X/O4FX2yldsMWkokkdmZL/4unJ3z9dbth+xJJn0OKu4l8+CFsMS04pEPWrlVcBj/91EZnQwMkJZHtoxz1v3jlzkhlU3Uk+006z6RtEFT9nsSusiH8/e/KqnbiRFixQsnuayoO101lNL+zY0Nlp9fu2gVhpCJCw1pX4egiUVf34wQRqH7ehPWeHWxlKhMnmmVoieSyRYq7CZw6pQjxtddi1CrVGJpE+bff2uhMToaGBpKslaP+URefbh8wAIOnF2PZw759xtkzGOChhQW4NBQTMT+ap5+Gv/wFvv8e7r+/a8+gnjoZa3SUbNrXnMusokJJgfPBB0pgThO//QYR6lRsh5hvCR0cDLtspxGYsgXrhhr2OU0zVy0OieSypVNxz8rKYsqUKURFRREdHc1bb70FQElJCdOnTycsLIzp06dTWloKgBCChx9+mNDQUGJjYzl40ELVei4Bb7yhpDUfPBhuuEFJYNhdmt6eNsX9kJKUa0dVHIMGgaPjRf0qFerrr2Oeej2Hd9cYZe///g9OrVM2UycuN5MCjhoFQFjp7yQlKU0rV8J77ykfhn5+MGmi4IEHYNcvdQQYzqIyo39EpYLswYoz3YAK/YTJXS54LZH0FTr9J2BlZcVrr71GcnIye/fu5e233yY5OZkVK1Ywbdo0UlNTmTZtGitWrABg48aNpKamkpqaysqVK1m+fLnFH6InKCxUsuzecYdSea5fP2UFn5vb9TENhmb9Zt8+0GovuuDgQXBy4peM0NYumSaWLMHZUIHPb992mgH47beVTAPLJzRGyrQ7qIm4uaENi2QMe/n5Z+W53n0Xrh5bzdk/vUOuSzjvH4znf6sNOBacQY0wv/N70iQMqDhIHCOudjfv2BLJZUin4u7v709cnOLzdXZ2JjIykpycHNavX8/ixYsBWLx4MevWrQNg/fr1LFq0CJVKxZgxYygrKyMvz0KFlHuQd99VNj//eHcZvqUp/PyvFBzLsvnww66PmZamlCedNQtqa88LfTOHDmGIHcbJVHX7OjxxIuWeA5lX8TFZWe3b+uwzJRJm9my4MSIJPDzA17frk78I6/FjGKfZy0+bBJs2QcOZLL5PHkT/FQ/g7dJARPVByr7+hb2rGyNlzCzukeM8eIPHeJNHmTTJrENLJJclJn15zcjI4NChQ4wePZr8/Hz8G9Pr+fn5kZ+vVKLJycmhf//+zfcEBgaS08bxxZUrVxIfH098fDyFhYXdeQaLU1cH//kPXDfLQPgtwyEykoHXRnKWAex7e3/7kS6d0OSSeegh5XcL14xeD4cPUxI8HJ2ug0W2Wk3V/MVMYwvHNpxt1d3QAI88ArfdBhOv0vH5JzrUyY1pB8xZyXnMGDz1hWTtSOe11+Bep8+wKy9QvuakpICPD6r//BuXgjTlejOLe1wc/JHX+M75doYONevQEsllidHiXlVVxY033sibb76Jy0UhFSqVCpWJQrFs2TISExNJTEzE29vbpHt7mi+/VNwyf5u1FzIy4Mkn4bPP0Dm4sKjgH2za1LVxDxxQfPhXX61sCrYQ99RUqK4mzVn51tTRBqHX44tQI9B8trpFe0MD3D9yP/3/9UfOeUTy6y5rHN2sFUPmcsk0MXo0AMMb9rJ1KyxxWQvx8Upgua0t3HsvbNighAV5eNB81NZMhIYqkT7jxoGVlVmHlkguS4z6Z6DVarnxxhu57bbbmD9/PgC+vr7k5eXh7+9PXl4ePj4+AAQEBJB1gX8gOzubgMv8WODvv4OrK8Slr1XU+JlnwMUFzcEj3PjPf3DfG+lcd93Azge6iIMHYV5YEtbXPc4NQ9/ni98GIoQiyhv+eoj5wLPrhqPRQHh4++PYRgzkgPNkYhI/BvF084p836epvHd0LCqNGs2IyXDVQkX51Gr4wx+69F60S3Q0wtGRcbV72asaT2Dufnjo5fP9994LL72khBk1fhCYE7VacT0NGGD2oSWSy5JOV+5CCJYuXUpkZCSPP/54c/ucOXNISEgAICEhgblz5za3r169GiEEe/fuxdXVtdl9c7mSkgKREQLVt98oy+zGby6aRx8CtZqYLW+SnW3amEIo4v7X8j/C5s08ePZJzp2D9HTFN37664M0qGywi4vijTeUxW9HJI9aQkDdafQ7zi//S//7LVboqT90An7+GZ57Tvlg+stfzF/r08oK1ciRzPLYy6tjv1XaGhcCgHLuv+m1hU4SXXedUntEIpHQeT73nTt3CkDExMSIoUOHiqFDh4oNGzaIoqIiMXXqVBEaGiqmTZsmiouLhRBCGAwGcf/994tBgwaJIUOGiP37O8/13dvzufv7C/Hs7ANCgBCrVrXoq5i/SFTiKF75U4lJY54+LcR4dihjRkYKAWI8O8Q11yhNqUHThDDhffns/QpRjb0ovOm+5rYD9uNEqvMwk+bVLf70JyGsrIQYNUqIIUNa9+9ofN7nn++5OUkkfZiOtFMW6+iEsjJFj36b8hchNBohCgtbXnDkiBAg3vR7yaRx13xlENuZIBo8/YQoLBSGwEBxUD1CqNCLmTMMwuDuLsQ99xg9Xl6eEJ+pbhVVdh5C1NeLrAP5Qo9K7L7mWZPm1S3WrVPeLBDi2TbsGgxCfPSRMlmJRNJtZLGObnCysQZETOo3MGmSUmv0QmJjOTNwKtee+y+VFZ0Eml9Axdc/M5Gd8NdnwMsL1csvM9xwgG+dF/N90VhUpaVKCIiR+PlBxlW34VhXQsN3mzj91g+oEfjcM9foMbrNhb70C10yTahUsGSJaWkfJRJJl5Di3kRRUZvNKSkQSTLO2SltCxZQd90Cwkjj+NoUo0zpquuZsOEpcmyCsV5+j9L4hz9gGDWGuZX/w0ptgL/9DW6/3aRHGP3XayjEi7x/forD5vVkawYwaP4wk8boFn5+ii8/NFQ6vyWSS4wUd4CkJOVAzzfftOo6c6SSt1SPItTqdgsg97tvDgC1X6xvs18IKCtTvgWsXAmfBjxJWPURts5+U4m+AVCrUW/ZDAUFynHV//f/wMnJpMeYPN2an1wX4rfvO6LzNnMqfA4qtRlj2Y3hvfeUhDLmjKGXSCQmI8UdlPA8g0EJ1bvwDP+5c9zx38lMEVtRffBBu2V83KIDOGYXj//+1uKu0ym5aNzdISICfr73axaX/5u02Y9y21cXuUycnKAbMf9qNahuvw1bUYcDtVjfOKfLY3WZGTNg8uSetyuRSFpweYp7ba0SM5iejlEVIjpjxw7l94ED5/8uKoJx4/AvT2HF2O/grrs6HCI1ci6DS39H5J1r2Z6qpBm48074bkUyXzguRYwaRejXr1gkudWMZ8dwhkGU40LMg/IcvkRypXJ5ivvVVyv14AYNUlbT33/f9bEMBvTbd7LBeSEVNl7kPPEa1ZUGuOMORHY216i3UDP52s6HuX4OagRFH7Wcy9Gjyu9nQ//H7BdGYeVgi+qrr867Y8yMl7eKn+e/x8fjPsDNxzI2JBJJ7+fyE/eMDNi9GxYvpuJfH1PoMRj9I49BfRdKzQGcOIGmtJhvqmey0voBAg58z8YB98KmTRT8+U1+048hIqLzYQbfGEM6wTSsaemaOXZU8J7qPoKevgNGjFBOLgUFdW2uRnLf2uk8sutmi9qQSCS9m8tP3Bs3PXPu+isj/7OY2wvfQJN+Wsln2wXqNituGOfrJvJY6v3orGxZUPYhZ8cu5Pfh9wEYJe7RQ1T8aDUXn2O/QHV1c3vFrqPcK96HBx9USjgFBnZpnhKJRGIKl6W414UPZcxtIeTng9P8GfzILHTPPt9uOGNHZP1vB9kEcMufB6Lx90Hz5BMk2cZxe81KTqQoER8d5XVpQqOB00PmYq2vhx9+aG4ffPgr9CqNEv0iM1pJJJIe4vIS97w8xO7dfFh6I1qtsve5ejW85vdPqKrC8OzfTBpOGATOR3aS5DGR0WMUIVe99CJ7/pXIziMu/Pe/4O+vJA0zBoeZE0knGP3b7wJQXiaYWfElmSFTuxUFI5FIJKZyeYn7unWohODdgvn87W8QG6uUnnvg7Sj+x+3oVn3cRjmj9vntf+n46XJwmzOxRVj2HYtU+PoqNVONcck0MfoqDe9wP5qd2+HoUdK/OUQop6mctdD4QSQSicQMXF7ivnYtBR7hnNJEsWDB+eZ58yBr6Gxs6qto2LHX6OEOvqn42+Memdii3c7ufAGNyEjjpzd+PHxidRdaKzt4+23El1+ixQqPpW0ffpJIJBJLcfmIe3ExYts2vtTeyKxrVXh6nu9SqeCqZ6aiR03au5uNGq68HLwPb6bK3guboa0VfPlyJTe4Kedx3N1h9LWerLG5DfHJJwT/9hm/Wk0nMNbD+EEkEonEDFw+4v7DD6j0ej6unN9mnYnJ89w5YjMS1S9GiHt1NYVz7uJW8RnV18xv86i8hwdkZsJNN5k2zdtug1drHkRVW4t7dTb7Bi6UJ/ElEkmPc/mI+8aNlNv7cdIhjjltnKrXaKB81DUMLt9H9rHS9sc5dgxGjmTQjo95y/kZvL/qWghle8yeDWech3HKZzz12FA6oQezMkokEkkjvUvc29sM1esRP//MBv1M5s1X4eDQ9mXhD05Hg4E9L/3aulMIeP99GDUKQ0kp11lvJv2uF1DbmDc80d4ebrwRbiz9kHl8S2i8m1nHl0gkEmPoPeKemYnOw5vqO+5Vsm1dyL59qEpLWdcwq8PSn/3mj6FG40TDhs0YDBd1LlsG990HEyey/rnDbNJO48Ybzf4UgJKpN0kbzkauJTbWMjYkEomkI3qNuBc98ybqqgoc/7eS7R7zeP2F6uYEjWLjJvSoOTPwaqZP72AQa2tKYqcwunIz27df0F5YCB9+CPfcAxs38ukvvvj5wVVXWeZZJk9W4uMBhgyxjA2JRCLpiN4h7no9Tl9+yOeq2/hx9juMr/yRMf9vOgkfNABQ8vkmfmc0j73g0ekhT5/bpxPKaTa9c+Z847Ztyu8776S6Vs2PPyp1NzQayzyORgOPPgrTphl/AEoikUjMSa8Qd1FQiJ22ikNTn+Da75aj+vRTrmIPBQ8+z+m9hbin7eeg90xuuaXzsWyuuwYA/Q8bz+cS27IFvaMzT68byciRSsbgC+PkLcH//R/88otlbUgkEkl79A5xzy/gF6Yx9fFhAKj/cAtVNy3hce0KfpnyImoEUY/PNG6lPXgwlQOimFv3BT/9pDTpft7Cz3WTePV1K/z9lWpIsp6ERCLpy/QKcVfrtXzs+UdmzDjf5vTBm9R79uPeurcotfJi8h/jjRtMpcLh7tuYwC42f5gJZ89ilZ7GZsNUjhxREjPec4+sAieRSPo2vULca7EjaNmMlitzV1ccP18FgHrmNaitjJ+q5g4lpMZj02fkf74VAIfrpxEVZbYpSyQSSa9GJcSFRUMvDbaqGI6nHiM0tI3O9euVDGEDB5o0ZnnMeLKTSklzjmNs5U9UnDxH6OBe8VkmkUgkZiE+Pp7ExMQ2+3qF2tk42bYt7ABz55os7ADOy28nmmSmV64lc+AUKewSieSKolcoXliY+cdUL7wJvdoKB2oZePc08xuQSCSSXkyvEHe1JWbh6QkzZwHgtVCKu0QiubLo03XfNC8+D6PjYdCgSz0ViUQi6VH6tLgzbJjyI5FIJFcYnTpE7rrrLnx8fBhyQZKUkpISpk+fTlhYGNOnT6e0VEmxK4Tg4YcfJjQ0lNjYWA4ePGi5mUskEomkXToV9yVLlrBp06YWbStWrGDatGmkpqYybdo0VqxYAcDGjRtJTU0lNTWVlStXsnz5csvMWiKRSCQd0qm4T5w4EQ+PlmXi1q9fz+LFiwFYvHgx69ata25ftGgRKpWKMWPGUFZWRl5envlnLZFIJJIO6VKcSn5+Pv6NOW39/PzIz88HICcnh/79+zdfFxgYSE5OTptjrFy5kvj4eOLj4yksLOzKNCQSiUTSDt0OQlSpVKi6kKhl2bJlJCYmkpiYiLe3d3enIZFIJJIL6JK4+/r6Nrtb8vLy8PHxASAgIICsrKzm67KzswkICDDDNCUSiURiCl0S9zlz5pCQkABAQkICc+fObW5fvXo1Qgj27t2Lq6trs/tGIpFIJD1Hp3Hut956K9u2baOoqIjAwED+9re/8ac//Ymbb76ZVatWERQUxFdffQXAtddey48//khoaCgODg589NFHFn8AiUQikbSmV2SF9PLyIjg4uEVbeXk5rh3UqOtOf2FhYYd+/svVdmf3StvmH1vaNt22Jf99XWm2MzIyKCoqavsG0Uu55557LNY/YsSIPmm7s3ulbfOPLW2bbtuS/76uZNsX0ysSh7XF7NmzLdrfF213x6603bWxpW3z2pW2zWjbpI+CPkJnKxppW9qWtqXty832xfTalbslWbZsmbQtbUvb0nafsn0xvWJDVSKRSCTm5YpcuUskEklfR4q7RCKR9EH6jLi3lXf+yJEjjB07lpiYGGbPnk1FRQWgxIba29szbNgwhg0bxn333dd8z9NPP03//v1xcnLqUds1NTVcd911REREEB0dzZ/+9Kcefe6ZM2cydOhQoqOjue+++9Dr9T1mu4k5c+a0GMvSdidPnkx4eHhzX0FBQY/ZbmhoYNmyZQwePJiIiAjWrl3bI7YrKyub24YNG4aXlxePPvpojz33559/TkxMDLGxscycObP9GG0L2P7yyy+JjY0lOjqap556qlO7ptoGOHr0KGPHjiU6OpqYmBjq6uoAOHDgADExMYSGhvLwww/TI97wS7yhaza2b98uDhw4IKKjo5vb4uPjxbZt24QQQqxatUo888wzQggh0tPTW1x3IXv27BG5ubnC0dGxR21XV1eLrVu3CiGEqK+vF+PHjxc//vhjj9gWQojy8nIhhBAGg0HMnz9ffP755z1mWwgh1q5dK2699dYOrzG33UmTJon9+/d3as8Stv/f//t/4umnnxZCCKHX60VhYWGP2b6QuLg4sX379h6xrdVqhbe3d/OzPvnkk+LZZ5/tEdtFRUWif//+oqCgQAghxKJFi8Qvv/xiVttarVbExMSIw4cPN9vU6XRCCCFGjhwp9uzZIwwGg5g5c6ZR/7a7S58RdyFa/4d1cXERBoNBCCHE2bNnRWRkZJvXtYUp4m5u20II8fDDD4uVK1f2uO2GhgZx/fXXiy+++KLHbFdWVopx48aJ48ePG/XemMtuV8TdXLYDAwNFVVXVJbHdxMmTJ0VgYGDz/Za23dDQILy8vERGRoYwGAzi3nvvFe+//36P2N63b5+YOnVq8+vVq1eL5cuXm9X2hg0bxG233dbq/tzcXBEeHt78+rPPPhPLli0zynZ36DNumbaIjo5m/fr1AKxZs6ZFxsr09HSGDx/OpEmT2LlzZ6+yXVZWxvfff8+0adN61PaMGTPw8fHB2dmZBQsW9Jjtv/71rzzxxBM4ODh0yWZX7QLceeedDBs2jBdeeKHLX5VNtV1WVgYozx0XF8dNN93UXBPB0rYv5IsvvmDhwoVdStndFdvW1ta8++67xMTE0K9fP5KTk1m6dGmP2A4NDeXkyZNkZGSg0+lYt25di3vMYfvUqVOoVCpmzJhBXFwcr776KqDUuQgMDGy+v6M6F2bF4h8fPcjFn7AnTpwQ06dPF3FxceK5554THh4eQggh6urqRFFRkRBCiMTERBEYGNjslmiiuyv3rtrWarVi5syZ4o033rgkz11bWyvmz58vfv755x6xfejQITF79uw2x7L0M2dnZwshhKioqBDTp08XCQkJPWK7sLBQAGLNmjVCCCFee+01cfvtt/fYczcRGRkpEhMTjbJrDtsNDQ1i6tSpIi0tTRgMBvHAAw+IF154ocee+7vvvhOjRo0SY8aMEY8//riYO3euWW3/4x//EMHBwaKwsFBUV1eLMWPGiF9++UXs379fTJs2rfn+HTt2iOuuu84o292hT4v7hZw8eVKMHDmyzb62vp53V9y7avvOO+8UDz300CWx3URCQoJ44IEHesT2O++8I/z9/UVQUJAICAgQ1tbWYtKkSRa3ezEfffRRjz2zwWAQDg4OQq/XCyGUr/ZRUVE9YruJw4cPi7CwMKNsmsv2xa6R7du3i1mzZvWI7Yt5//33xZNPPmlW259//rlYtGhRc9/zzz8vXn31VemWsQRN0Q8Gg4G///3vzTvnhYWFzdEgZ86cITU1lUGDBl1y28888wzl5eW8+eabPWq7qqqqufiKTqdjw4YNRERE9Ijt5cuXk5ubS0ZGBrt27WLw4MFs27bN4nZ1Ol1zpIZWq+WHH34wKlLHHLZVKhWzZ89ufs4tW7YQFRXVI7ab+Pzzz7n11lu7ZLOrtgMCAkhOTm4uq7l582YiIyN7xPaF95SWlvLOO+9w9913m9X2jBkzOHbsGDU1Neh0OrZv305UVBT+/v64uLiwd+9ehBCsXr26uQaGRbH4x0cPccsttwg/Pz9hZWUlAgICxIcffijefPNNERYWJsLCwsRTTz3VvAny9ddfi6ioKDF06FAxfPhw8d133zWP8+STT4qAgAChUqlEQECAUbv55rCdlZUlABERESGGDh0qhg4dKj744IMesX3u3DkRHx8vYmJiRHR0tHjwwQeFVqvtsfe8CWPdMuawW1VVJeLi4kRMTIyIiooSDz/8cHNkQ088c0ZGhpgwYYKIiYkRU6dOFZmZmT36fg8cOFCcOHGiU5vmtv3uu++KiIgIERMTI66//vpmF0pP2L7llltEZGSkiIyMNCoazFTbQgjxySefiKioKBEdHd3im8H+/ftFdHS0GDRokHjggQeM3sTuDjL9gEQikfRB+rRbRiKRSK5UpLhLJBJJH0SKu0QikfRBpLhLJBJJH0SKu0QikfRBpLhLJBJJH0SKu0QikfRB/j/njzbQmuoFUwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # df = pd.read_csv('AirPassengers.csv', encoding='utf-8', index_col='date')\n",
    "    # df.index = pd.to_datetime(df.index)\n",
    "    # ts = df['x']\n",
    "    df = pd.read_csv('AirPassengers.csv', encoding='utf-8')\n",
    "    df.columns = (['date','x']) \n",
    "    ts = df.set_index(pd.DatetimeIndex(df['date']))['x'] \n",
    "    #      freq  \n",
    "    ts.index= pd.DatetimeIndex(ts.index.values,freq=ts.index.inferred_freq)\n",
    "\n",
    " \n",
    "    #   \n",
    "    ts_log = np.log(ts)\n",
    "    rol_mean = ts_log.rolling(window=12).mean()\n",
    "    rol_mean.dropna(inplace=True)\n",
    "    ts_diff_1 = rol_mean.diff(1)\n",
    "    ts_diff_1.dropna(inplace=True)\n",
    "    ts_diff_2 = ts_diff_1.diff(1)\n",
    "    ts_diff_2.dropna(inplace=True)\n",
    " \n",
    "    #  \n",
    "    model = arima_model(ts_diff_2)\n",
    "    #        \n",
    "    # model.get_proper_model() # pq\n",
    "    model.get_proper_model(0,1) \n",
    "    print ('bic:', model.bic, 'p:', model.p, 'q:', model.q)\n",
    "    print (model.properModel.forecast()[0])\n",
    "    print (model.forecast_next_day_value(type='month'))\n",
    " \n",
    "    #   \n",
    "    predict_ts = model.properModel.predict()\n",
    "    diff_shift_ts = ts_diff_1.shift(1)\n",
    "    diff_recover_1 = predict_ts.add(diff_shift_ts)\n",
    "    rol_shift_ts = rol_mean.shift(1)\n",
    "    diff_recover = diff_recover_1.add(rol_shift_ts)\n",
    "    rol_sum = ts_log.rolling(window=11).sum()\n",
    "    rol_recover = diff_recover*12 - rol_sum.shift(1)\n",
    "    log_recover = np.exp(rol_recover)\n",
    "    log_recover.dropna(inplace=True)\n",
    " \n",
    "    #   \n",
    "    ts = ts[log_recover.index]\n",
    "    plt.figure(facecolor='white')\n",
    "    log_recover.plot(color='blue', label='Predict')\n",
    "    ts.plot(color='red', label='Original')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('RMSE: %.4f'% np.sqrt(sum((log_recover-ts)**2)/ts.size))\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae9660db38ac8643b5abe3ade23e36bb7e77026bbf1f37e511afc8ce3b66621b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
